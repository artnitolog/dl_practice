{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Третье практическое задание. Реализация дропаута в рекуррентных нейронных сетях\nПрактикум на ЭВМ для 317 группы, весна 2021\n\n#### Фамилия, имя: Васильев Руслан\n\nДата выдачи: 3 апреля 00:01\n\nМягкий дедлайн: 17 апреля 00:01","metadata":{}},{"cell_type":"markdown","source":"Данное задание будет состоять из двух частей:\n1. Применение рекуррентной сети для решения задачи классификации текста. Более конкретно -- предсказания рейтинга отзыва фильма.\n2. Простейшая лингвистическая модель для генерации текста на основе LSTM.","metadata":{}},{"cell_type":"markdown","source":"При выполнении задания вы обучите LSTM с разным уровнем \"коробочности\", а также познакомитесь с различными способами применения DropOut к рекуррентным архитектурам. В рекуррентных архитектурах вариантов, куда можно наложить бинарную маску шума, гораздо больше, чем в нейросетях прямого прохода.\n\nВо второй части вы попробуете реализовать простейший рекуррентный декодер для генерации текстов.\n\nЗадание сделано так, чтобы его можно было выполнять на CPU, однако RNN - это ресурсоёмкая вещь, поэтому на GPU с ними работать приятнее. Можете попробовать использовать [https://colab.research.google.com](https://colab.research.google.com) - бесплатное облако с GPU.","metadata":{}},{"cell_type":"markdown","source":"**Для корректного отображения картинок, вам может понадобится сделать ноутбук доверенным (Trusted) в правом верхнем углу**","metadata":{}},{"cell_type":"markdown","source":"# Часть 0. Загрузка и предобработка данных. (1 балл)","metadata":{}},{"cell_type":"markdown","source":"## Рекомендуемые гиперпараметры","metadata":{}},{"cell_type":"code","source":"max_length = 200\ntop_n_words = 5000\n\nhidden_dim = 128\nembedding_dim = 32\n\nnum_epochs = 15\nbatch_size = 64\nlearning_rate = 1e-3","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:04:28.454122Z","start_time":"2021-04-02T00:04:28.438278Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Первое, что нужно сделать -- скачать, предобработать данные и организовать их таким образом, чтобы их можно было подавать в нейронную сеть.\n\nДля обеих частей задания мы будем использовать [**Large Movie Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/).","metadata":{}},{"cell_type":"markdown","source":"## Загрузка и предобработка данных","metadata":{}},{"cell_type":"markdown","source":"Загрузите данные по ссылке выше. (**tip**: используйте `wget`)","metadata":{}},{"cell_type":"code","source":"!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz","metadata":{"ExecuteTime":{"end_time":"2021-03-30T18:34:47.261797Z","start_time":"2021-03-30T18:34:33.768597Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2021-04-19 19:11:59--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\nResolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\nConnecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 84125825 (80M) [application/x-gzip]\nSaving to: ‘aclImdb_v1.tar.gz’\n\naclImdb_v1.tar.gz   100%[===================>]  80.23M  68.8MB/s    in 1.2s    \n\n2021-04-19 19:12:00 (68.8 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Распакуйте скачанные данные в папку `aclImdb` (**tip:** используйте `tar`)","metadata":{}},{"cell_type":"code","source":"!tar --extract --file=aclImdb_v1.tar.gz\n!tree aclImdb -L 2","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"aclImdb\n├── README\n├── imdb.vocab\n├── imdbEr.txt\n├── test\n│   ├── labeledBow.feat\n│   ├── neg\n│   ├── pos\n│   ├── urls_neg.txt\n│   └── urls_pos.txt\n└── train\n    ├── labeledBow.feat\n    ├── neg\n    ├── pos\n    ├── unsup\n    ├── unsupBow.feat\n    ├── urls_neg.txt\n    ├── urls_pos.txt\n    └── urls_unsup.txt\n\n7 directories, 11 files\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Посмотрите в файле `./aclImdb/README` как организованы данные","metadata":{}},{"cell_type":"code","source":"! cat ./aclImdb/train/pos/10003_8.txt","metadata":{"ExecuteTime":{"end_time":"2021-04-01T23:55:43.946032Z","start_time":"2021-04-01T23:55:43.814779Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his characteristic speaking to directly to the audience. Again, Warren was the best actor in the movie, but \"Fume\" and \"Sailor\" both played their parts well.","output_type":"stream"}]},{"cell_type":"code","source":"test_data_path = './aclImdb/test/'\ntrain_data_path = './aclImdb/train/'","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:04:33.729663Z","start_time":"2021-04-02T00:04:33.710871Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nfrom functools import partial\nfrom collections import defaultdict\n\nimport nltk\nnltk.download('stopwords')\n\nimport regex\nimport numpy as np\n\nimport torch\nimport torchtext\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:04:33.709378Z","start_time":"2021-04-02T00:04:32.22058Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Стандартной предобработкой данных является токенизация текстов. Полученные токены можно будет закодировать и затем подавать на вход нейронной сети. Ключевым моментом, который влияет на скорость работы нейросети и её размер в памяти -- размер словаря, используемого при токенизации. Для задачи классификации мы можем убрать часть слов (стоп слова, редкие слова), ускорив обучение без потери в качестве.","metadata":{}},{"cell_type":"code","source":"STOPWORDS = nltk.corpus.stopwords.words('english')","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:04:35.270825Z","start_time":"2021-04-02T00:04:35.250283Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Реализуйте функцию для токенизации текста. Выполнять токенизацию можно по-разному, но в данном задании предлагается это делать следующим образом:\n1. Привести текст к нижнему регистру\n2. Убрать html разметку из текстов (`<br />`)\n3. Убрать все символы кроме латинских букв\n4. Разбить строку по пробелам\n5. Убрать стоп слова","metadata":{}},{"cell_type":"code","source":"def tokenize(text):\n    \"\"\"\n    :param str text: Input text \n    :return List[str]: List of words\n    \"\"\"\n    text = text.lower()\n    text = regex.sub('<br />', '', text)\n    return [w for w in regex.findall('[a-z]+', text) if w not in STOPWORDS]","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:04:36.003194Z","start_time":"2021-04-02T00:04:35.980408Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenize('1. Hello <br /> words!! <br />')","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:27:12.428149Z","start_time":"2021-04-01T21:27:12.402448Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['hello', 'words']"},"metadata":{}}]},{"cell_type":"markdown","source":"Теперь мы можем создать словарь, с помощью которого мы будем численно кодировать токены из текста и наоборот.\n\nУдобной обёрткой для создания словарей является класс `torchtext.vocab.Vocab`.","metadata":{}},{"cell_type":"code","source":"# torchtext.vocab.Vocab??","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:27:21.466887Z","start_time":"2021-04-01T21:27:21.352085Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Чтобы создать такой словарь, сначала нужно создать словарь со всеми токенами в тексте и их частотами встречаемости:","metadata":{"ExecuteTime":{"end_time":"2021-04-01T19:51:55.300753Z","start_time":"2021-04-01T19:51:55.275188Z"}}},{"cell_type":"code","source":"counter = defaultdict(int)\n\nfor path in ['./aclImdb/test/neg', './aclImdb/test/pos', './aclImdb/train/neg', './aclImdb/train/pos']:\n    for file_path in os.listdir(path):\n        text = open(os.path.join(path, file_path), 'r', encoding='utf-8', errors='ignore').read().strip()\n        for token in tokenize(text):\n            counter[token] += 1","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Для работы с текстами нам необходимо зарезервировать два специальных токена:\n1. `<pad>` для токена означающего паддинг\n2. `<unk>` для токенов, которые отсутствуют в словаре","metadata":{}},{"cell_type":"code","source":"specials = ['<pad>', '<unk>']\nfor special in specials:\n    counter[special] = 0","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:28:18.239274Z","start_time":"2021-04-01T21:28:18.214979Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Создайте словарь из словаря частот `counter`. Наименьшие id отдайте под специальные токены. Отбросьте низкочастотные слова, оставив только `top_n_words` слов","metadata":{}},{"cell_type":"code","source":"from torchtext.vocab import Vocab\n\nvocab = Vocab(counter, specials=specials, specials_first=True, max_size=top_n_words)","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:28:23.041153Z","start_time":"2021-04-01T21:28:22.899444Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"vocab.lookup_indices(['<pad>', '<unk>'])","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:28:24.332126Z","start_time":"2021-04-01T21:28:24.30689Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0, 1]"},"metadata":{}}]},{"cell_type":"code","source":"vocab.lookup_indices(['this', 'film', 'was', 'awful'])","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:28:26.25491Z","start_time":"2021-04-01T21:28:26.231012Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[1, 3, 1, 254]"},"metadata":{}}]},{"cell_type":"markdown","source":"Теперь мы готовы создать обёртку-датасет для наших данных. \n\nНеобходимо добавить несколько опции, которые понадобятся во второй части задания:\n1. Ограничение на максимальную длину текста в токенах. Если текст оказывается длиннее, то последние токены отбрасываются\n2. Возможность добавить в специальные токены `<sos>`, `<eos>` в начало и конец токенизированного текста\n    \n**tips:**\n1. Обратите особое внимание, что у длинных текстов не должен обрезаться паддинг\n2. В исходных данных рейтинг закодирован в названии файла в виде числа от 1 до 10. Для удобства, вычтите 1, чтобы рейтинг был от 0 до 9","metadata":{}},{"cell_type":"code","source":"def i2t(nums):\n    return torch.tensor(nums, dtype=torch.long)\n\nclass LargeMovieReviewDataset(Dataset):\n    def __init__(self, data_path, vocab, max_len, pad_sos=False, pad_eos=False):\n        \"\"\"\n        :param str data_path: Path to folder with one of the data splits (train or test)\n        :param torchtext.vocab.Vocab vocab: dictionary with lookup_indices method\n        :param int max_len: Maximum length of tokenized text\n        :param bool pad_sos: If True pad sequence at the beginning with <sos> \n        :param bool pad_eos: If True pad sequence at the end with <eos>         \n        \"\"\"\n        super().__init__()\n                \n        self.pad_sos = pad_sos\n        if self.pad_sos:\n            self.tok_pref = vocab.lookup_indices(['<sos>'])\n        else:\n            self.tok_pref = []\n        self.pad_eos = pad_eos\n        if self.pad_eos:\n            self.tok_suf = vocab.lookup_indices(['<eos>'])\n        else:\n            self.tok_suf = []\n        \n        self.vocab = vocab\n        self.max_len = max_len\n        self.data_path = data_path\n        self.negative_path = os.path.join(data_path, 'neg')\n        self.positive_path = os.path.join(data_path, 'pos')\n        \n        self.negative_paths = []\n        self.positive_paths = []\n\n        for file_path in os.listdir(self.negative_path):\n            self.negative_paths.append(os.path.join(self.negative_path, file_path))\n\n        for file_path in os.listdir(self.positive_path):\n            self.positive_paths.append(os.path.join(self.positive_path, file_path))\n        \n        self.texts = []\n        self.tokens = []\n        self.ratings = []\n        self.labels = [0] * len(self.negative_paths) + [1] * len(self.positive_paths)\n        \n        # Read each file in data_path, tokenize it, get tokens ids, its rating and store\n        for path in self.negative_paths + self.positive_paths:\n            with open(path, 'r') as fin:\n                text = fin.read()\n            self.texts.append(text)\n            # self.tokens.append(tokenize(text))\n            self.tokens.append(vocab.lookup_indices(tokenize(text)))\n            self.ratings.append((int(path[-5]) + 9) % 10)\n        \n    def __getitem__(self, idx):\n        \"\"\"\n        :param int idx: index of object in dataset\n        :return dict: Dictionary with all useful object data \n            {\n                'text' str: unprocessed text,\n                'label' torch.tensor(dtype=torch.long): sentiment of the text (0 for negative, 1 for positive)\n                'rating' torch.tensor(dtype=torch.long): rating of the text\n                'tokens' torch.tensor(dtype=torch.long): tensor of tokens ids for the text\n                'tokens_len' torch.tensor(dtype=torch.long): number of tokens\n            }\n        \"\"\"\n        item = {\n            'text': self.texts[idx],\n            'label':  i2t(self.labels[idx]),\n            'rating': i2t(self.ratings[idx]),\n            'tokens': i2t(self.tok_pref + self.tokens[idx][:self.max_len] + self.tok_suf),\n        }\n        item['tokens_len'] = i2t(len(item['tokens']) - len(self.tok_pref) - len(self.tok_suf))\n        return item\n    \n    def __len__(self):\n        \"\"\"\n        :return int: number of objects in dataset \n        \"\"\"\n        return len(self.texts)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:13.573249Z","start_time":"2021-04-02T00:05:13.548593Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Создайте датасеты для тестовой и обучающей выборки. \n\nОбратите внимание, что для задачи классификации нам не потребуется паддинг с помощью `<sos>`, `<eos>`. \n\nНе забудьте обрезать длинные тексты, передав параметр `max_length`.","metadata":{}},{"cell_type":"code","source":"test_dataset = LargeMovieReviewDataset(train_data_path, vocab, max_length)\ntrain_dataset = LargeMovieReviewDataset(test_data_path, vocab, max_length)","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:31:32.123618Z","start_time":"2021-04-01T21:30:54.950259Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, как выглядит объект в датасете:","metadata":{}},{"cell_type":"code","source":"test_dataset[0]","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:31:36.017106Z","start_time":"2021-04-01T21:31:35.988797Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'text': \"and a 30,000$ budget and this movie still looks like it was made for 50$. You can tell from the first frame to the last that he didn't care one bit about the movies continuity or plot, he was just happy to be making a zombie movie. <br /><br />What the end result shows is a lazy film maker who loves zombie movies. It could have been great if he just had of given a care. The end result is endless zoom ins on poorly done gore, and even more poorly produced metal plays over it.<br /><br />What happens when you combine high hopes, big dreams, a decent budget, hard work, and one idiot behind the camera.\",\n 'label': tensor(0),\n 'rating': tensor(1),\n 'tokens': tensor([ 214,    2,   48,  175,    5,   21,  258,   20, 1791,  128,  332,    4,\n          118,   25, 2273,   36,  529,  124,  851,    2,   46,  794,  162, 2780,\n            3, 2544, 1209,  851,   25,   24,   18,  242,  332,   46,  794, 2005,\n            1,    1,  721,  114,  468,    8,  721,  882, 2521,  185,  421, 4352,\n          184, 1814,   87, 1301,  406,  214,  141,   64,    4, 2276,  384,  234]),\n 'tokens_len': tensor(60)}"},"metadata":{}}]},{"cell_type":"markdown","source":"Теперь нам нужно создать `DataLoader` для наших данных. `DataLoader` умеет из коробки объединять список объектов из датасета в один батч, даже когда датасет возвращает словарь тензоров. Однако, это работает только в случае когда все эти тензоры имеют один и тот же размер во всех батчах. В нашем случае, это не так, так как разные тексты могут иметь разную длину.\n\nЧтобы обойти эту проблему у `DataLoader` есть параметр `collate_fn`, который позволяет задать функцию для объединения списка объектов в один батч.","metadata":{}},{"cell_type":"markdown","source":"Чтобы объединить несколько тензоров разной длины в один можно использовать функцию `torch.nn.utils.rnn.pad_sequence`\n\nОбратите внимание на её аргументы:\n1. `batch_first` определяет по какой оси \"складывать\" тензоры. Предпочтительнее использовать `batch_first=False` так как это может упростить выполнение задания в дальнейшем \n2. `padding_value` -- число, которое будет использоваться в качестве паддинга, чтобы сделать все тензоры одинаковой длины","metadata":{}},{"cell_type":"code","source":"torch.nn.utils.rnn.pad_sequence([\n    torch.tensor([1, 2, 3]),\n    torch.tensor([4, 5]),\n    torch.tensor([6, 7, 8, 9])\n], batch_first=False, padding_value=-1)","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:33:06.372403Z","start_time":"2021-04-01T21:33:06.344559Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([[ 1,  4,  6],\n        [ 2,  5,  7],\n        [ 3, -1,  8],\n        [-1, -1,  9]])"},"metadata":{}}]},{"cell_type":"code","source":"from collections import defaultdict\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch, padding_value, batch_first=False):\n    \"\"\"\n    :param List[Dict] batch: List of objects from dataset\n    :param int padding_value: Value that will be used to pad tokens\n    :param bool batch_first: If True resulting tensor with tokens must have shape [B, T] otherwise [T, B]\n    :return dict: Dictionary with all data collated\n        {\n            'ratings' torch.tensor(dtype=torch.long): rating of the text for each object in batch\n            'labels' torch.tensor(dtype=torch.long): sentiment of the text for each object in batch\n            \n            'texts' List[str]: All texts in one list\n            'tokens' torch.tensor(dtype=torch.long): tensor of tokens ids padded with @padding_value\n            'tokens_lens' torch.tensor(dtype=torch.long): number of tokens for each object in batch\n        }\n    \"\"\"\n    concat = defaultdict(list)\n    for dict_obj in batch:\n        for key, data in dict_obj.items():\n            concat[key].append(data)\n    return {\n        'ratings': torch.stack(concat['rating']),\n        'labels': torch.stack(concat['label']),\n        'texts': concat['text'],\n        'tokens': pad_sequence(concat['tokens'], batch_first, padding_value),\n        'tokens_lens': torch.stack(concat['tokens_len']),\n    }","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:13.709466Z","start_time":"2021-04-02T00:05:13.575053Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Создайте даталоадеры с использованием `collate_fn`.\n\n**tips**:\n1. Передать в `collate_fn` правильное значение паддинга можно, например, с помощью `functools.partial`\n2. Если вы работаете в Google Colab, то, возможно, вам будет необходимо установить `num_workers=0` во избежание падения ноутбука.","metadata":{}},{"cell_type":"code","source":"from functools import partial\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn = partial(collate_fn, padding_value=vocab.stoi['<pad>']),\n    num_workers=0,\n    shuffle=True,\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    collate_fn = partial(collate_fn, padding_value=vocab.stoi['<pad>']),\n    num_workers=0,\n    shuffle=False,\n)","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:33:37.87276Z","start_time":"2021-04-01T21:33:37.847071Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на какой-нибудь батч:","metadata":{}},{"cell_type":"code","source":"batch = next(iter(test_dataloader))\nbatch.keys(), batch['labels'], batch['ratings'], batch['tokens'], batch['tokens_lens']","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:33:46.922744Z","start_time":"2021-04-01T21:33:46.755275Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(dict_keys(['ratings', 'labels', 'texts', 'tokens', 'tokens_lens']),\n tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n tensor([1, 0, 0, 0, 1, 2, 3, 0, 1, 3, 3, 3, 1, 2, 0, 2, 0, 0, 1, 2, 3, 2, 0, 0,\n         0, 2, 2, 0, 3, 0, 2, 0, 1, 1, 0, 2, 2, 3, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0,\n         3, 0, 1, 1, 1, 2, 0, 0, 3, 2, 0, 0, 2, 2, 1, 2]),\n tensor([[ 214,    1,    1,  ...,   43,   32, 2244],\n         [   2,    1,    1,  ..., 3282,   30,  322],\n         [  48,    4,    1,  ..., 1291,   33, 1664],\n         ...,\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0],\n         [   0,    0,    0,  ...,    0,    0,    0]]),\n tensor([ 60,  48,  62,  80,  64,  81,  29, 113, 200,  43, 170, 200, 115, 125,\n          51,  87,  70,  66, 200, 200, 101, 100, 107, 103,  71, 164,  64,  84,\n          89,  75,  51,  94, 108,  74, 200,  62, 200,  52,  83, 200,  67,  75,\n         200,  89, 104, 146,  28,  66, 117,  29,  66,  23, 133, 115,  63, 103,\n          81,  65,  71,  84,  53,  45, 137,  72]))"},"metadata":{}}]},{"cell_type":"markdown","source":"# Часть 1. Классификация текстов. (6 баллов)","metadata":{}},{"cell_type":"markdown","source":"## Сборка и обучение RNN в pytorch (2 балла)","metadata":{}},{"cell_type":"markdown","source":"Создадим переменные для device-agnostic кода:","metadata":{}},{"cell_type":"code","source":"dtype, device, cuda_device_id = torch.float32, None, 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '{0}'.format(str(cuda_device_id) if cuda_device_id is not None else '')\nif cuda_device_id is not None and torch.cuda.is_available():\n    device = 'cuda:{0:d}'.format(0)\nelse:\n    device = torch.device('cpu')\nprint(f'Using device: {device}, dtype: {dtype}')","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:13.809416Z","start_time":"2021-04-02T00:05:13.711383Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Using device: cuda:0, dtype: torch.float32\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Наша нейросеть будет обрабатывать входную последовательность по словам (word level). Мы будем использовать простую и стандартную рекуррентную архитектуру для классификации:\n1. Слой представлений, превращающий id токена в вектор-ембеддинг этого слова\n2. Слой LSTM\n3. Полносвязный слой, предсказывающий выход по последнему скрытому состоянию\n\nНиже дан код для сборки и обучения нашей нейросети.","metadata":{}},{"cell_type":"markdown","source":"Допишите класс-обёртку над LSTM для задачи классификации. \n**Не используйте циклы.**","metadata":{}},{"cell_type":"markdown","source":"**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**","metadata":{"ExecuteTime":{"end_time":"2021-04-01T20:59:16.467178Z","start_time":"2021-04-01T20:59:16.441112Z"}}},{"cell_type":"code","source":"class RNNClassifier(torch.nn.Module):\n    def __init__(\n        self, embedding_dim, hidden_dim, output_size, vocab,\n        rec_layer=torch.nn.LSTM, dropout=None, **kwargs\n    ):\n        super().__init__()\n\n        self.dropout = dropout\n        \n        self.vocab = vocab\n        self.hidden_dim = hidden_dim\n        self.output_size = output_size\n        self.embedding_dim = embedding_dim\n        \n        # Create a simple lookup table that stores embeddings of a fixed dictionary and size.\n        #    Use torch.nn.Embedding. Do not forget specify padding_idx!\n        self.word_embeddings = torch.nn.Embedding(\n            num_embeddings=len(vocab),\n            embedding_dim=embedding_dim,\n            padding_idx=vocab.stoi['<pad>'],\n        )\n        \n        if dropout is not None:\n            self.rnn = rec_layer(self.embedding_dim, self.hidden_dim, dropout=self.dropout, **kwargs)\n        else:\n            self.rnn = rec_layer(self.embedding_dim, self.hidden_dim, **kwargs)\n        \n        # Create linear layer for classification\n        self.lin = torch.nn.Linear(hidden_dim, output_size)\n\n    def forward(self, tokens, tokens_lens):\n        \"\"\"\n        Args:\n            tokens: torch.tensor(dtype=torch.long) of shape (seq_len, batch).\n                Batch of texts represented with tokens.\n            tokens_lens: torch.tensor(dtype=torch.long) of shape (batch,).\n                Number of non-padding tokens for each object in batch.\n        Returns:\n            output: torch.tensor(dtype=torch.float32) of shape (batch, output_size).\n                Vector representation for each sequence in batch.\n        \"\"\"\n        # Evaluate embeddings\n        output = self.word_embeddings(tokens)  # (seq_len, batch, embedding_dim)\n        \n        # Make forward pass through recurrent network\n        output = self.rnn(output)[0]  # (seq_len, batch, hidden_dim)\n\n        # Pass output from rnn to linear layer \n        # Note: each object in batch has its own length \n        #     so we must take rnn hidden state after the last token for each text in batch\n        output = output[tokens_lens - 1, torch.arange(len(tokens_lens))]  # (batch, hidden_dim)\n        output = self.lin(output)  # (batch, output_size)\n        return output","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:13.873713Z","start_time":"2021-04-02T00:05:13.81069Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"[Исходный код LSTM](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM)","metadata":{}},{"cell_type":"markdown","source":"Допишите функции для обучения и оценки модели:\n\n**tip:**\n1. В функции `evaluate` при подсчёте метрик учитывайте, что батчи могут иметь разный размер. (в частности последний батч)","metadata":{}},{"cell_type":"code","source":"def extract_batch(data, device):\n    return [data[key].to(device) for key in ['tokens', 'tokens_lens', 'ratings']]\n\ndef train_epoch(dataloader, model, loss_fn, optimizer, device):\n    model.train()\n    for idx, data in enumerate(dataloader):\n        tokens, tokens_lens, ratings = extract_batch(data, device)\n        model.zero_grad()\n        output = model(tokens, tokens_lens)\n        loss = loss_fn(output, ratings)\n        loss.backward()\n        optimizer.step()\n\ndef evaluate(dataloader, model, loss_fn, device):\n    model.eval()\n    total_loss, total_acc = 0.0, 0\n    with torch.no_grad():\n        for idx, data in enumerate(dataloader):\n            tokens, tokens_lens, ratings = extract_batch(data, device)\n            output = model(tokens, tokens_lens)\n            total_loss += loss_fn(output, ratings).item() * len(ratings)\n            total_acc += (output.argmax(axis=1) == ratings).sum().item()\n    return total_loss / len(dataloader.dataset), total_acc / len(dataloader.dataset)\n\n\ndef train(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs):\n    train_losses, test_losses = [], []\n    train_accs, test_accs = [], []\n    for epoch in range(1, num_epochs+1):\n        train_epoch(train_loader, model, loss_fn, optimizer, device)\n        train_loss, train_acc = evaluate(train_loader, model, loss_fn, device)\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        test_loss, test_acc = evaluate(test_loader, model, loss_fn, device)\n        test_losses.append(test_loss)\n        test_accs.append(test_acc)\n        print(f'epoch {epoch:02}/{num_epochs};', end=' ')\n        print(f'loss (train/test): {train_loss:.3f}/{test_loss:.3f};', end=' ')\n        print(f'accuracy (train/test): {train_acc:.3f}/{test_acc:.3f}')\n    return train_losses, train_accs, test_losses, test_accs","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:13.969665Z","start_time":"2021-04-02T00:05:13.875379Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Создадим модель:","metadata":{}},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=torch.nn.LSTM, dropout=None,\n).to(device)","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:39:36.315652Z","start_time":"2021-04-01T21:39:33.667776Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Создадим класс для подсчёта функции потерь и оптимизатор:","metadata":{}},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:39:38.737281Z","start_time":"2021-04-01T21:39:38.711948Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Попробуем обучить модель:","metadata":{}},{"cell_type":"markdown","source":"**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**","metadata":{}},{"cell_type":"code","source":"from time import time\nevals = {}\ntimes = {}\nstart = time()\nevals['torch.nn.LSTM'] = train(train_dataloader, test_dataloader, model,\n                               loss_fn, optimizer, device, num_epochs)\ntimes['torch.nn.LSTM'] = time() - start","metadata":{"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 1.931/1.953; accuracy (train/test): 0.286/0.273\nepoch 02/15; loss (train/test): 1.671/1.740; accuracy (train/test): 0.361/0.340\nepoch 03/15; loss (train/test): 1.556/1.655; accuracy (train/test): 0.393/0.362\nepoch 04/15; loss (train/test): 1.488/1.620; accuracy (train/test): 0.417/0.379\nepoch 05/15; loss (train/test): 1.398/1.574; accuracy (train/test): 0.449/0.390\nepoch 06/15; loss (train/test): 1.357/1.587; accuracy (train/test): 0.464/0.393\nepoch 07/15; loss (train/test): 1.309/1.585; accuracy (train/test): 0.483/0.388\nepoch 08/15; loss (train/test): 1.257/1.580; accuracy (train/test): 0.505/0.392\nepoch 09/15; loss (train/test): 1.219/1.610; accuracy (train/test): 0.519/0.393\nepoch 10/15; loss (train/test): 1.141/1.647; accuracy (train/test): 0.557/0.391\nepoch 11/15; loss (train/test): 1.098/1.719; accuracy (train/test): 0.570/0.378\nepoch 12/15; loss (train/test): 1.000/1.754; accuracy (train/test): 0.621/0.376\nepoch 13/15; loss (train/test): 0.926/1.816; accuracy (train/test): 0.653/0.374\nepoch 14/15; loss (train/test): 0.837/1.944; accuracy (train/test): 0.695/0.365\nepoch 15/15; loss (train/test): 0.754/2.076; accuracy (train/test): 0.729/0.361\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Нерегуляризованные LSTM часто быстро переобучаются (и мы это видим по точности на контроле). Чтобы с этим бороться, часто используют L2-регуляризацию и дропаут.\nОднако способов накладывать дропаут на рекуррентный слой достаточно много, и далеко не все хорошо работают. По [ссылке](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b) доступен хороший обзор дропаутов для RNN.\n\nМы реализуем два варианта DropOut для RNN (и третий дополнительно). Заодно увидим, что для реализации различных усовершенствований рекуррентной архитектуры приходится \"вскрывать\" слой до различной \"глубины\".","metadata":{}},{"cell_type":"markdown","source":"## Реализация дропаута по статье Гала и Гарамани. Variational Dropout. (1 балл)","metadata":{}},{"cell_type":"markdown","source":"Начнем с дропаута, описанного в [статье Гала и Гарамани](https://arxiv.org/abs/1512.05287).\nДля этого нам потребуется перейти от использования слоя `torch.nn.LSTM`, полностью скрывающего от нас рекуррентную логику, к использованию слоя `torch.nn.LSTMCell`, обрабатывающего лишь один временной шаг нашей последовательности (а всю логику вокруг придется реализовать самостоятельно). \n\nДопишите класс `RNNLayer`. При `dropout=0` ваш класс должен работать как обычный слой LSTM, а при `dropout > 0` накладывать бинарную маску на входной и скрытый вектор на каждом временном шаге, причем эта маска должна быть одинаковой во все моменты времени.\n\nДропаут Гала и Гарамани в виде формул (m обозначает маску дропаута):\n\n$$\nh_{t-1} = h_{t-1}*m_h, \\, x_t = x_t * m_x\n$$\n\nДалее обычный шаг рекуррентной архитектуры, например, LSTM:\n\n$$\ni = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\no = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n$$\n$$\nf = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \ng = tanh(h_{t-1} W^g + x_t U^g+b_g) \n$$\n$$\nc_t = f \\odot c_{t-1} +  i \\odot  g \\quad\nh_t =  o \\odot tanh(c_t) \\nonumber\n$$","metadata":{}},{"cell_type":"code","source":"def init_h0_c0(num_objects, hidden_size, some_existing_tensor):\n    \"\"\"\n    return h0 and c0, use some_existing_tensor.new_zeros() to gen them\n    h0 shape: num_objects x hidden_size\n    c0 shape: num_objects x hidden_size\n    \"\"\"\n    size = (num_objects, hidden_size)\n    h0 = some_existing_tensor.new_zeros(size)\n    c0 = some_existing_tensor.new_zeros(size)\n    return h0, c0","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:14.068954Z","start_time":"2021-04-02T00:05:13.971286Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def gen_dropout_mask(input_size, hidden_size, is_training, p, other):\n    \"\"\"\n    is_training: if True, gen masks from Bernoulli\n                 if False, gen masks consisting of (1-p)\n    \n    return dropout masks of size input_size, hidden_size if p is not None\n    return one masks if p is None\n    \"\"\"\n    if p is None:\n        mx = other.new_ones(input_size)\n        mh = other.new_ones(hidden_size)\n    elif is_training:\n        mx = other.new_empty(input_size).bernoulli_(1 - p)\n        mh = other.new_empty(hidden_size).bernoulli_(1 - p)\n    else:\n        mx = other.new_full((input_size,), 1 - p)\n        mh = other.new_full((hidden_size,), 1 - p)\n    return mx, mh","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:14.131347Z","start_time":"2021-04-02T00:05:14.071577Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Допишите класс-обёртку над LSTMCell для реализации Variational Dropout. **Используйте только цикл по времени**","metadata":{"ExecuteTime":{"end_time":"2021-04-01T21:09:12.282613Z","start_time":"2021-04-01T21:09:12.256019Z"}}},{"cell_type":"markdown","source":"**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**","metadata":{}},{"cell_type":"code","source":"class RNNLayer(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.0):\n        super().__init__()\n\n        self.dropout = dropout\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.rnn_cell = torch.nn.LSTMCell(self.input_size, self.hidden_size)\n    \n    def forward(self, input):\n        '''\n        Args:\n            input: tensor of shape (seq_len, batch, input_size).\n        Returns: output, (h_n, c_n)\n            output: tensor of shape (seq_len, batch, hidden_size)\n            h_n: tensor of shape (batch, hidden_size)\n            c_n: tensor of shape (batch, hidden_size)\n        '''\n        # Initialize h=h_0, c=c_0 of shape (batch, hidden_size)\n        h, c = init_h0_c0(input.size()[1], self.hidden_size, input)\n        \n        # Gen masks for input and hidden state\n        if self.dropout > 0.0:\n            # mx (input_size,), mh (hidden_size,)\n            mx, mh = gen_dropout_mask(self.input_size, self.hidden_size,\n                                      self.training, self.dropout, input)\n            # masked_input (seq_len, batch, input_size).\n            masked_input = input * mx\n        else:\n            masked_input = input\n        \n        # Implement recurrent logic and return what nn.LSTM returns\n        # Do not forget to apply generated dropout masks!\n        output = []\n        for x in masked_input:\n            # x: (batch, input_size)\n            # h and c: (batch, hidden_size)\n            h, c = self.rnn_cell(x, (h, c))\n            if self.dropout > 0.0:\n                h = h * mh\n            output.append(h)\n        output = torch.stack(output, dim=0)\n        return output, (h, c)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:14.190066Z","start_time":"2021-04-02T00:05:14.132804Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Протестируйте реализованную модель с выключенным дропаутом (слой `RNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Сильно ли оно увеличилось по сравнению с `torch.nn.LSTM` (LSTM \"из коробки\")?","metadata":{}},{"cell_type":"markdown","source":"**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**","metadata":{}},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=RNNLayer,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nstart = time()\nevals['RNNLayer'] = train(train_dataloader, test_dataloader, model,\n                          loss_fn, optimizer, device, num_epochs)\ntimes['RNNLayer'] = time() - start","metadata":{"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 1.999/2.012; accuracy (train/test): 0.253/0.234\nepoch 02/15; loss (train/test): 1.683/1.739; accuracy (train/test): 0.359/0.338\nepoch 03/15; loss (train/test): 1.578/1.681; accuracy (train/test): 0.387/0.363\nepoch 04/15; loss (train/test): 1.461/1.595; accuracy (train/test): 0.428/0.383\nepoch 05/15; loss (train/test): 1.396/1.570; accuracy (train/test): 0.452/0.390\nepoch 06/15; loss (train/test): 1.333/1.590; accuracy (train/test): 0.475/0.392\nepoch 07/15; loss (train/test): 1.275/1.589; accuracy (train/test): 0.495/0.399\nepoch 08/15; loss (train/test): 1.225/1.626; accuracy (train/test): 0.515/0.390\nepoch 09/15; loss (train/test): 1.157/1.636; accuracy (train/test): 0.542/0.391\nepoch 10/15; loss (train/test): 1.093/1.710; accuracy (train/test): 0.575/0.378\nepoch 11/15; loss (train/test): 1.031/1.730; accuracy (train/test): 0.605/0.374\nepoch 12/15; loss (train/test): 0.960/1.820; accuracy (train/test): 0.633/0.372\nepoch 13/15; loss (train/test): 0.860/1.884; accuracy (train/test): 0.685/0.366\nepoch 14/15; loss (train/test): 0.786/2.012; accuracy (train/test): 0.707/0.370\nepoch 15/15; loss (train/test): 0.688/2.157; accuracy (train/test): 0.759/0.357\n","output_type":"stream"}]},{"cell_type":"code","source":"mtimes = times['RNNLayer'] / times['torch.nn.LSTM']\nprint(f\"Время обучения с RNNLayer увеличилось в {mtimes:.2f} \"\n      f\"раз по сравнению с torch.nn.LSTM\")","metadata":{"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Время обучения с RNNLayer увеличилось в 3.27 раз по сравнению с torch.nn.LSTM\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=blue>Здесь и дале сравнения во времени рабоы показываются в выводе (print).</font>","metadata":{}},{"cell_type":"markdown","source":"Протестируйте полученную модель с `dropout=0.25`, вновь замерив время обучения. Получилось ли побороть переобучение? Сильно ли дольше обучается данная модель по сравнению с предыдущей? (доп. время тратится на генерацию масок дропаута).","metadata":{}},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=RNNLayer, dropout=0.25,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\ntimes_do = {}\nstart = time()\nevals['RNNLayer + VarDO'] = train(train_dataloader, test_dataloader, model,\n                                    loss_fn, optimizer, device, num_epochs)\ntimes_do['RNNLayer + VarDO'] = time() - start","metadata":{"ExecuteTime":{"end_time":"2021-04-01T22:18:28.301613Z","start_time":"2021-04-01T22:04:19.10785Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 1.996/2.005; accuracy (train/test): 0.263/0.250\nepoch 02/15; loss (train/test): 1.835/1.870; accuracy (train/test): 0.326/0.307\nepoch 03/15; loss (train/test): 1.674/1.729; accuracy (train/test): 0.361/0.344\nepoch 04/15; loss (train/test): 1.614/1.678; accuracy (train/test): 0.376/0.354\nepoch 05/15; loss (train/test): 1.553/1.636; accuracy (train/test): 0.386/0.361\nepoch 06/15; loss (train/test): 1.530/1.622; accuracy (train/test): 0.399/0.367\nepoch 07/15; loss (train/test): 1.494/1.600; accuracy (train/test): 0.412/0.374\nepoch 08/15; loss (train/test): 1.464/1.586; accuracy (train/test): 0.417/0.382\nepoch 09/15; loss (train/test): 1.443/1.580; accuracy (train/test): 0.433/0.388\nepoch 10/15; loss (train/test): 1.405/1.551; accuracy (train/test): 0.445/0.392\nepoch 11/15; loss (train/test): 1.397/1.549; accuracy (train/test): 0.443/0.395\nepoch 12/15; loss (train/test): 1.371/1.540; accuracy (train/test): 0.459/0.399\nepoch 13/15; loss (train/test): 1.348/1.536; accuracy (train/test): 0.467/0.400\nepoch 14/15; loss (train/test): 1.343/1.543; accuracy (train/test): 0.466/0.401\nepoch 15/15; loss (train/test): 1.314/1.543; accuracy (train/test): 0.477/0.402\n","output_type":"stream"}]},{"cell_type":"code","source":"mtimes = times_do['RNNLayer + VarDO'] / times['RNNLayer']\nprint(f\"Модель с RNNLayer с дропаутом обучается в {mtimes:.2f} \"\n      f\"раз дольше, чем без него\")","metadata":{"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Модель с RNNLayer с дропаутом обучается в 1.16 раз дольше, чем без него\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=blue>Использование `RNNLayer` замедляет время обучения по сравнению со `torch.nn.LSTM`, а использование замедляет слой еще чуть больше. Тем не менее по качеству на тесте видим, что переобучение удалось побороть.</font>","metadata":{}},{"cell_type":"markdown","source":"## Реализация дропаута по статье Гала и Гарамани. Дубль 2 (1 балл)","metadata":{}},{"cell_type":"markdown","source":"<начало взлома pytorch>","metadata":{}},{"cell_type":"markdown","source":"При разворачивании цикла по времени средствами python обучение рекуррентной нейросети сильно замедляется. Однако для реализации дропаута Гала и Гарамани необязательно явно задавать в коде домножение нейронов на маски. Можно схитрить и обойтись использованием слоя `torch.nn.LSTM`: перед вызовом `forward` слоя `torch.nn.LSTM` подменять его веса на веса, домноженные по строкам на маски. А обучаемые веса хранить отдельно. Именно так этот дропаут реализован в библиотеке `fastai`, код из которой использован в ячейке ниже.\n\nТакой слой реализуется в виде обертки над `torch.nn.LSTM`. Допишите класс:","metadata":{}},{"cell_type":"code","source":"import warnings","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:14.286206Z","start_time":"2021-04-02T00:05:14.19173Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class FastRNNLayer(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.0,\n                 layers_dropout=0.0, num_layers=1):\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        self.num_layers = num_layers\n        \n        self.dropout = dropout\n        self.layers_dropout = layers_dropout\n        self.module = torch.nn.LSTM(input_size, hidden_size, dropout=layers_dropout,\n                                    num_layers=num_layers)\n\n        self.layer_names = []\n        for layer_n in range(self.num_layers):\n            self.layer_names += [f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}']\n\n        for layer in self.layer_names:\n            # Get torch.nn.Parameter with weights from torch.nn.LSTM instance\n            w = getattr(self.module, layer)\n\n            # Remove it from model\n            delattr(self.module, layer)\n\n            # And create new torch.nn.Parameter with the same data but different name\n            self.register_parameter(f'{layer}_raw', torch.nn.Parameter(w.data))\n\n            # Note. In torch.nn.LSTM.forward parameter with name `layer` will be used\n            #     so we must initialize it using `layer_raw` before forward pass\n    \n    def _setweights(self, x):\n        \"\"\"\n            Apply dropout to the raw weights.\n        \"\"\"\n        for layer in self.layer_names:\n            # Get torch.nn.Parameter with weights\n            raw_w = getattr(self, f'{layer}_raw')\n            \n            if self.dropout > 0.0:\n                # Generate mask (use function gen_dropout_mask)\n                mask = gen_dropout_mask(raw_w.size()[1], 0, self.training,\n                                        self.dropout, raw_w)[0]\n                # Apply dropout mask\n                masked_raw_w = raw_w * mask\n            else:\n                masked_raw_w = raw_w\n\n            # Set modified weights in its place\n            setattr(self.module, layer, masked_raw_w)\n\n    def forward(self, x, h_c=None):\n        \"\"\"\n        :param x: tensor containing the features of the input sequence.\n        :param Optional[Tuple[torch.tensor, torch.tensor]] h_c: initial hidden state and initial cell state\n        \"\"\"\n        with warnings.catch_warnings():\n            # To avoid the warning that comes because the weights aren't flattened.\n            warnings.simplefilter(\"ignore\")\n\n            # Set new weights of self.module and call its forward\n            # Pass h_c with x if it is not None. Otherwise pass only x\n            self._setweights(x)\n            output = self.module(x, h_c)  # h_c=None by default\n        return output\n            \n    def reset(self):\n        if hasattr(self.module, 'reset'):\n            self.module.reset()","metadata":{"ExecuteTime":{"end_time":"2021-04-02T02:06:54.953123Z","start_time":"2021-04-02T02:06:54.917017Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Протестируйте реализованную модель с выключенным дропаутом (слой `FastRNNLayer` надо передать в `RNNClassifier` в качестве `rec_layer`). Замерьте время обучения. Убедитесь, что модель выдаёт такое же качество, как и оригинальная реализация LSTM.","metadata":{}},{"cell_type":"markdown","source":"**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**","metadata":{}},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=FastRNNLayer,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nstart = time()\nevals['FastRNNLayer'] = train(train_dataloader, test_dataloader, model,\n                              loss_fn, optimizer, device, num_epochs)\ntimes['FastRNNLayer'] = time() - start","metadata":{"ExecuteTime":{"end_time":"2021-04-01T22:25:23.843846Z","start_time":"2021-04-01T22:22:43.059254Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 1.953/1.969; accuracy (train/test): 0.268/0.256\nepoch 02/15; loss (train/test): 1.687/1.741; accuracy (train/test): 0.356/0.336\nepoch 03/15; loss (train/test): 1.655/1.719; accuracy (train/test): 0.369/0.344\nepoch 04/15; loss (train/test): 1.502/1.615; accuracy (train/test): 0.415/0.372\nepoch 05/15; loss (train/test): 1.461/1.602; accuracy (train/test): 0.429/0.383\nepoch 06/15; loss (train/test): 1.370/1.565; accuracy (train/test): 0.452/0.394\nepoch 07/15; loss (train/test): 1.331/1.569; accuracy (train/test): 0.471/0.393\nepoch 08/15; loss (train/test): 1.285/1.598; accuracy (train/test): 0.491/0.386\nepoch 09/15; loss (train/test): 1.201/1.596; accuracy (train/test): 0.522/0.391\nepoch 10/15; loss (train/test): 1.143/1.634; accuracy (train/test): 0.547/0.391\nepoch 11/15; loss (train/test): 1.076/1.680; accuracy (train/test): 0.576/0.387\nepoch 12/15; loss (train/test): 1.017/1.763; accuracy (train/test): 0.601/0.383\nepoch 13/15; loss (train/test): 0.917/1.836; accuracy (train/test): 0.652/0.371\nepoch 14/15; loss (train/test): 0.848/1.924; accuracy (train/test): 0.679/0.369\nepoch 15/15; loss (train/test): 0.757/2.000; accuracy (train/test): 0.726/0.361\n","output_type":"stream"}]},{"cell_type":"code","source":"mtimes = [times['FastRNNLayer'] / times['torch.nn.LSTM'], \n          times['RNNLayer'] / times['FastRNNLayer']]\nprint(f\"Модель с FastRNNLayer медленнее сети с torch.nn.LSTM \"\n      f\"в {mtimes[0]:.2f} раз и быстрее сети с RNNLayer \"\n      f\"в {mtimes[1]:.2f} раз\")","metadata":{"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Модель с FastRNNLayer медленнее сети с torch.nn.LSTM в 1.03 раз и быстрее сети с RNNLayer в 3.18 раз\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=blue>По времени работы `FastRNNLayer` почти не отличается от `torch.nn.LSTM`, качество не отличается.</font>","metadata":{}},{"cell_type":"markdown","source":"Протестируйте полученный слой (вновь подставив его в `RNNClassifier` в качестве `rec_layer`) с `dropout=0.25`. Сравните время обучения с предыдущими моделями. Проследите, чтобы качество получилось такое же, как при первой реализации этого дропаута.","metadata":{}},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=FastRNNLayer, dropout=0.25,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nstart = time()\nevals['FastRNNLayer + VarDO'] = train(train_dataloader, test_dataloader, model,\n                                        loss_fn, optimizer, device, num_epochs)\ntimes_do['FastRNNLayer + VarDO'] = time() - start","metadata":{"ExecuteTime":{"end_time":"2021-04-01T22:28:38.168777Z","start_time":"2021-04-01T22:25:56.717326Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 2.001/2.012; accuracy (train/test): 0.238/0.230\nepoch 02/15; loss (train/test): 1.800/1.832; accuracy (train/test): 0.339/0.323\nepoch 03/15; loss (train/test): 1.670/1.719; accuracy (train/test): 0.359/0.346\nepoch 04/15; loss (train/test): 1.605/1.660; accuracy (train/test): 0.377/0.359\nepoch 05/15; loss (train/test): 1.559/1.630; accuracy (train/test): 0.391/0.367\nepoch 06/15; loss (train/test): 1.519/1.607; accuracy (train/test): 0.399/0.376\nepoch 07/15; loss (train/test): 1.490/1.587; accuracy (train/test): 0.410/0.382\nepoch 08/15; loss (train/test): 1.440/1.555; accuracy (train/test): 0.432/0.391\nepoch 09/15; loss (train/test): 1.425/1.557; accuracy (train/test): 0.437/0.393\nepoch 10/15; loss (train/test): 1.382/1.533; accuracy (train/test): 0.453/0.404\nepoch 11/15; loss (train/test): 1.364/1.533; accuracy (train/test): 0.461/0.403\nepoch 12/15; loss (train/test): 1.331/1.523; accuracy (train/test): 0.474/0.409\nepoch 13/15; loss (train/test): 1.331/1.533; accuracy (train/test): 0.476/0.404\nepoch 14/15; loss (train/test): 1.298/1.524; accuracy (train/test): 0.485/0.412\nepoch 15/15; loss (train/test): 1.269/1.525; accuracy (train/test): 0.496/0.413\n","output_type":"stream"}]},{"cell_type":"code","source":"mtimes = times_do['FastRNNLayer + VarDO'] / times['FastRNNLayer']\nprint(f\"Сеть с FastRNNLayer с дропаутом медленнее \"\n      f\"в {mtimes:.2f} раз, чем без него\")","metadata":{"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Сеть с FastRNNLayer с дропаутом медленнее в 1.02 раз, чем без него\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=blue>Что касается качества, accuracy почти не отличается от предыдущей реализации дропаута.</font>","metadata":{}},{"cell_type":"markdown","source":"</конец взлома pytorch>","metadata":{}},{"cell_type":"markdown","source":"## Реализация дропаута по статье Семениуты и др. (1 балл)","metadata":{}},{"cell_type":"markdown","source":"Перейдем к реализации дропаута для LSTM по статье [Semeniuta et al](http://www.aclweb.org/anthology/C16-1165). \n\nЭтот метод применения дропаута не менее популярен, чем предыдущий. Его особенность состоит в том, что он придуман специально для гейтовых архитектур. В контексте LSTM этот дропаут накладывается только на информационный поток (m_h - маска дропаута):\n$$\ni = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\no = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n$$\n$$\nf = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \ng = tanh(h_{t-1} W^g + x_t U^g+b_g) \n$$\n$$\nc_t = f \\odot c_{t-1} +  i \\odot g \\odot {\\bf m_h} \\quad\nh_t =  o \\odot tanh(c_t) \\nonumber\n$$\nНа входы $x_t$ маска накладывается как в предыдущем дропауте. Впрочем, на входы маску можно наложить вообще до вызова рекуррентного слоя.\n\nСогласно статье, маска дропаута может быть как одинаковая, так и разная для всех моментов времени. Мы сделаем одинаковую для всех моментов времени.\n\nДля реализации этого дропаута можно: \n1. самостоятельно реализовать LSTM (интерфейса LSTMCell не хватит) \n2. снова воспользоваться трюком с установкой весов (но тут мы опираемся на свойство $tanh(0)=0$, к тому же, трюк в данном случае выглядит менее тривиально, чем с дропаутом Гала). \n\nПредлагается реализовать дропаут по сценарию 1. Допишите класс:","metadata":{}},{"cell_type":"markdown","source":"**Для каждого тензора в функции `forward` подпишите в комментарии его размеры**","metadata":{}},{"cell_type":"code","source":"class HandmadeLSTM(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.0):\n        super().__init__()\n        \n        self.dropout = dropout\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.input_weights = torch.nn.Linear(input_size, 4 * hidden_size)\n        self.hidden_weights = torch.nn.Linear(hidden_size, 4 * hidden_size, bias=False)\n        \n        self.reset_params()\n\n    def reset_params(self):\n        \"\"\"\n        Initialization as in Pytorch. \n        Do not forget to call this method!\n        https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n        \"\"\"\n        stdv = 1.0 / np.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            torch.nn.init.uniform_(weight, -stdv, stdv)\n\n    def forward(self, input):\n        '''\n        Args:\n            input: tensor of shape (seq_len, batch, input_size).\n        Returns: output, (h_n, c_n)\n            output: tensor of shape (seq_len, batch, hidden_size)\n            h_n: tensor of shape (batch, hidden_size)\n            c_n: tensor of shape (batch, hidden_size)\n        '''\n        # Use functions init_h0_c0 and gen_dropout_masks defined above\n        # h, c of shape (batch, hidden_size)\n        h, c = init_h0_c0(input.size()[1], self.hidden_size, input)\n        \n        if self.dropout > 0.0:\n            # mx (input_size,), mh (hidden_size,)\n            mx, mh = gen_dropout_mask(self.input_size, self.hidden_size,\n                                      self.training, self.dropout, input)\n            # masked_input (seq_len, batch, input_size).\n            masked_input = input * mx\n        else:\n            masked_input = input\n        \n        # Implement recurrent logic to mimic torch.nn.LSTM\n        # Do not forget to apply dropout mask\n        output = []\n        for x in masked_input:\n            # lin (batch, 4 * hidden_size)\n            lin = self.input_weights(x) + self.hidden_weights(h)\n            # i_f_o (batch, 3 * hidden_size), g (batch, hidden_size)\n            i_f_o, g = lin.split(3 * self.hidden_size, dim=1)\n            # i, f, o, g: (batch, hidden_size)\n            i, f, o = torch.sigmoid(i_f_o).split(self.hidden_size, dim=1)\n            g = torch.tanh(g)\n            if self.dropout > 0.0:\n                g = g * mh\n            # c, h: (batch, hidden_size)\n            c = f * c + i * g\n            h = o * torch.tanh(c)\n            output.append(h)\n        output = torch.stack(output, dim=0)\n        return output, (h, c)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:05:14.447201Z","start_time":"2021-04-02T00:05:14.350457Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Протестируйте вашу реализацию без дропаута (проконтролируйте качество и сравните время обучения с временем обучения `torch.nn.LSTM` и `RNNLayer`), а также с `dropout=0.25`. Сравните качество модели с таким дропаутом с качеством модели с дропаутом Гала и Гарамани.","metadata":{}},{"cell_type":"markdown","source":"**Сохраните все метрики и время работы модели. Это потребуется в конце первой части для построения графиков обучения и сравнения времени работы для всех моделей в этой секции**","metadata":{}},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=HandmadeLSTM,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nstart = time()\nevals['HandmadeLSTM'] = train(train_dataloader, test_dataloader, model,\n                              loss_fn, optimizer, device, num_epochs)\ntimes['HandmadeLSTM'] = time() - start","metadata":{"ExecuteTime":{"end_time":"2021-04-01T23:01:53.046622Z","start_time":"2021-04-01T22:30:08.733475Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 1.895/1.913; accuracy (train/test): 0.303/0.291\nepoch 02/15; loss (train/test): 1.676/1.735; accuracy (train/test): 0.359/0.340\nepoch 03/15; loss (train/test): 1.538/1.638; accuracy (train/test): 0.402/0.368\nepoch 04/15; loss (train/test): 1.478/1.613; accuracy (train/test): 0.416/0.376\nepoch 05/15; loss (train/test): 1.419/1.601; accuracy (train/test): 0.440/0.382\nepoch 06/15; loss (train/test): 1.355/1.578; accuracy (train/test): 0.471/0.388\nepoch 07/15; loss (train/test): 1.298/1.596; accuracy (train/test): 0.481/0.397\nepoch 08/15; loss (train/test): 1.229/1.626; accuracy (train/test): 0.509/0.397\nepoch 09/15; loss (train/test): 1.175/1.623; accuracy (train/test): 0.537/0.389\nepoch 10/15; loss (train/test): 1.140/1.670; accuracy (train/test): 0.556/0.370\nepoch 11/15; loss (train/test): 1.073/1.740; accuracy (train/test): 0.573/0.385\nepoch 12/15; loss (train/test): 0.972/1.789; accuracy (train/test): 0.626/0.374\nepoch 13/15; loss (train/test): 0.899/1.888; accuracy (train/test): 0.664/0.365\nepoch 14/15; loss (train/test): 0.842/1.952; accuracy (train/test): 0.689/0.357\nepoch 15/15; loss (train/test): 0.743/2.052; accuracy (train/test): 0.725/0.366\n","output_type":"stream"}]},{"cell_type":"code","source":"mtimes = [times['HandmadeLSTM'] / times['torch.nn.LSTM'], \n          times['HandmadeLSTM'] / times['RNNLayer']]\nprint(f\"Модель с HandmadeLSTM медленнее сети с torch.nn.LSTM \"\n      f\"в {mtimes[0]:.2f} раз и сети с RNNLayer \"\n      f\"в {mtimes[1]:.2f} раз\")","metadata":{"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Модель с HandmadeLSTM медленнее сети с torch.nn.LSTM в 7.57 раз и сети с RNNLayer в 2.32 раз\n","output_type":"stream"}]},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=HandmadeLSTM, dropout=0.25,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nstart = time()\nevals['HandmadeLSTM + RecDO'] = train(train_dataloader, test_dataloader, model,\n                                      loss_fn, optimizer, device, num_epochs)\ntimes_do['HandmadeLSTM + RecDO'] = time() - start","metadata":{"ExecuteTime":{"end_time":"2021-04-01T23:33:28.808063Z","start_time":"2021-04-01T23:01:53.049547Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 1.996/2.006; accuracy (train/test): 0.259/0.253\nepoch 02/15; loss (train/test): 1.887/1.910; accuracy (train/test): 0.300/0.287\nepoch 03/15; loss (train/test): 1.696/1.746; accuracy (train/test): 0.354/0.340\nepoch 04/15; loss (train/test): 1.694/1.760; accuracy (train/test): 0.366/0.349\nepoch 05/15; loss (train/test): 1.576/1.650; accuracy (train/test): 0.381/0.359\nepoch 06/15; loss (train/test): 1.552/1.639; accuracy (train/test): 0.392/0.367\nepoch 07/15; loss (train/test): 1.515/1.612; accuracy (train/test): 0.403/0.376\nepoch 08/15; loss (train/test): 1.464/1.582; accuracy (train/test): 0.420/0.389\nepoch 09/15; loss (train/test): 1.444/1.572; accuracy (train/test): 0.426/0.393\nepoch 10/15; loss (train/test): 1.424/1.562; accuracy (train/test): 0.434/0.397\nepoch 11/15; loss (train/test): 1.396/1.560; accuracy (train/test): 0.441/0.399\nepoch 12/15; loss (train/test): 1.386/1.558; accuracy (train/test): 0.450/0.400\nepoch 13/15; loss (train/test): 1.362/1.548; accuracy (train/test): 0.456/0.398\nepoch 14/15; loss (train/test): 1.355/1.575; accuracy (train/test): 0.462/0.406\nepoch 15/15; loss (train/test): 1.339/1.572; accuracy (train/test): 0.465/0.406\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=blue>Качество почти не отличается от подхода Гала и Гарамани.</font>","metadata":{}},{"cell_type":"markdown","source":"## Сравнение всех предложенных моделей. (1 балл)","metadata":{"ExecuteTime":{"end_time":"2021-04-01T23:33:28.831346Z","start_time":"2021-04-01T23:33:28.810453Z"}}},{"cell_type":"markdown","source":"Используя замеры времени заполните табличку с временем работы четырёх реализованных моделей в следующей ячейке:","metadata":{"ExecuteTime":{"end_time":"2021-04-01T23:48:05.361634Z","start_time":"2021-04-01T23:48:05.333901Z"}}},{"cell_type":"code","source":"import pandas as pd\n\ndef sec2time(sec):\n    mm, ss = divmod(sec, 60)\n    return f'{int(mm)}m {round(ss)}s'\n\ndef timetable(time_dict):\n    df = pd.DataFrame(time_dict, index=['time']).applymap(sec2time)\n    return df.style.set_properties(**{'text-align': 'center'})","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"<font color=blue>Время работы моделей без дропаута:</font>","metadata":{}},{"cell_type":"code","source":"timetable(times)","metadata":{"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7eff70735d10>","text/html":"<style  type=\"text/css\" >\n#T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col0,#T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col1,#T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col2,#T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col3{\n            text-align:  center;\n        }</style><table id=\"T_d0c900b2_a14c_11eb_b979_0242ac130202\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >torch.nn.LSTM</th>        <th class=\"col_heading level0 col1\" >RNNLayer</th>        <th class=\"col_heading level0 col2\" >FastRNNLayer</th>        <th class=\"col_heading level0 col3\" >HandmadeLSTM</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_d0c900b2_a14c_11eb_b979_0242ac130202level0_row0\" class=\"row_heading level0 row0\" >time</th>\n                        <td id=\"T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col0\" class=\"data row0 col0\" >2m 40s</td>\n                        <td id=\"T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col1\" class=\"data row0 col1\" >8m 43s</td>\n                        <td id=\"T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col2\" class=\"data row0 col2\" >2m 44s</td>\n                        <td id=\"T_d0c900b2_a14c_11eb_b979_0242ac130202row0_col3\" class=\"data row0 col3\" >20m 12s</td>\n            </tr>\n    </tbody></table>"},"metadata":{}}]},{"cell_type":"markdown","source":"<font color=blue>А также измерения для моделей с дропаутом:</font>","metadata":{}},{"cell_type":"code","source":"timetable(times_do)","metadata":{"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7eff70f39a50>","text/html":"<style  type=\"text/css\" >\n#T_d0ce54b8_a14c_11eb_b979_0242ac130202row0_col0,#T_d0ce54b8_a14c_11eb_b979_0242ac130202row0_col1,#T_d0ce54b8_a14c_11eb_b979_0242ac130202row0_col2{\n            text-align:  center;\n        }</style><table id=\"T_d0ce54b8_a14c_11eb_b979_0242ac130202\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >RNNLayer + VarDO</th>        <th class=\"col_heading level0 col1\" >FastRNNLayer + VarDO</th>        <th class=\"col_heading level0 col2\" >HandmadeLSTM + RecDO</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_d0ce54b8_a14c_11eb_b979_0242ac130202level0_row0\" class=\"row_heading level0 row0\" >time</th>\n                        <td id=\"T_d0ce54b8_a14c_11eb_b979_0242ac130202row0_col0\" class=\"data row0 col0\" >10m 7s</td>\n                        <td id=\"T_d0ce54b8_a14c_11eb_b979_0242ac130202row0_col1\" class=\"data row0 col1\" >2m 47s</td>\n                        <td id=\"T_d0ce54b8_a14c_11eb_b979_0242ac130202row0_col2\" class=\"data row0 col2\" >20m 54s</td>\n            </tr>\n    </tbody></table>"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"\nКрайне желательно рисовать графики в векторном формате.\n\nЕсли по каким-то причинам, отрисовка не будет работать, закомментируйте следующую ячейку.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nfrom IPython.display import set_matplotlib_formats\n\nset_matplotlib_formats('pdf', 'svg')","metadata":{"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Нарисуйте два графика -- функция потерь и качество на обучающей и тестовой выборке для всех 7 моделей обученных выше.","metadata":{}},{"cell_type":"code","source":"plt.rc('axes', axisbelow=True, grid=True)\nplt.rc('grid', c='grey', ls=':')\nplt.rc('font', family='serif')","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nepochs = np.arange(num_epochs) + 1\n\nplt.subplot(211)\nplt.title('Cross-Entropy loss: solid — test, dashed — train')\nplt.xlabel('Epoch')\nfor name, (train_loss, _, test_loss, _) in evals.items():\n    p = plt.plot(epochs, train_loss, linestyle='--')\n    plt.plot(epochs, test_loss, label=name, color=p[0].get_color())\nplt.legend()\n\nplt.subplot(212)\nplt.title('Accuracy: solid — test, dashed — train')\nplt.xlabel('Epoch')\nfor name, (_, train_acc, _, test_acc) in evals.items():\n    p = plt.plot(epochs, train_acc, linestyle='--')\n    plt.plot(epochs, test_acc, label=name, color=p[0].get_color())\nplt.legend()\n\nplt.tight_layout()","metadata":{"ExecuteTime":{"end_time":"2021-04-01T23:40:37.809057Z","start_time":"2021-04-01T23:40:36.694896Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 2 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"713.274375pt\" version=\"1.1\" viewBox=\"0 0 712.578125 713.274375\" width=\"712.578125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-19T20:21:26.433948</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 713.274375 \nL 712.578125 713.274375 \nL 712.578125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 321.118125 \nL 705.378125 321.118125 \nL 705.378125 22.318125 \nL 30.103125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 104.646469 321.118125 \nL 104.646469 22.318125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc5363a1cd4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.646469\" xlink:href=\"#mc5363a1cd4\" y=\"321.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 2 -->\n      <g transform=\"translate(101.465219 335.716562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 819 3553 \nL 469 3553 \nL 469 4384 \nQ 803 4563 1142 4656 \nQ 1481 4750 1806 4750 \nQ 2534 4750 2956 4397 \nQ 3378 4044 3378 3438 \nQ 3378 2753 2422 1800 \nQ 2347 1728 2309 1691 \nL 1131 513 \nL 3078 513 \nL 3078 1088 \nL 3444 1088 \nL 3444 0 \nL 434 0 \nL 434 341 \nL 1850 1753 \nQ 2319 2222 2519 2614 \nQ 2719 3006 2719 3438 \nQ 2719 3909 2473 4175 \nQ 2228 4441 1797 4441 \nQ 1350 4441 1106 4219 \nQ 863 3997 819 3553 \nz\n\" id=\"DejaVuSerif-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 192.344521 321.118125 \nL 192.344521 22.318125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.344521\" xlink:href=\"#mc5363a1cd4\" y=\"321.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 4 -->\n      <g transform=\"translate(189.163271 335.716562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2234 1581 \nL 2234 4063 \nL 641 1581 \nL 2234 1581 \nz\nM 3609 0 \nL 1484 0 \nL 1484 331 \nL 2234 331 \nL 2234 1247 \nL 197 1247 \nL 197 1588 \nL 2241 4750 \nL 2859 4750 \nL 2859 1581 \nL 3750 1581 \nL 3750 1247 \nL 2859 1247 \nL 2859 331 \nL 3609 331 \nL 3609 0 \nz\n\" id=\"DejaVuSerif-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 280.042573 321.118125 \nL 280.042573 22.318125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"280.042573\" xlink:href=\"#mc5363a1cd4\" y=\"321.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 6 -->\n      <g transform=\"translate(276.861323 335.716562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2094 219 \nQ 2534 219 2771 542 \nQ 3009 866 3009 1472 \nQ 3009 2078 2771 2401 \nQ 2534 2725 2094 2725 \nQ 1647 2725 1412 2412 \nQ 1178 2100 1178 1509 \nQ 1178 888 1415 553 \nQ 1653 219 2094 219 \nz\nM 1075 2569 \nQ 1288 2803 1556 2918 \nQ 1825 3034 2163 3034 \nQ 2859 3034 3264 2615 \nQ 3669 2197 3669 1472 \nQ 3669 763 3233 336 \nQ 2797 -91 2069 -91 \nQ 1278 -91 853 498 \nQ 428 1088 428 2181 \nQ 428 3406 931 4078 \nQ 1434 4750 2350 4750 \nQ 2597 4750 2869 4703 \nQ 3141 4656 3425 4563 \nL 3425 3794 \nL 3072 3794 \nQ 3034 4109 2831 4275 \nQ 2628 4441 2284 4441 \nQ 1678 4441 1381 3981 \nQ 1084 3522 1075 2569 \nz\n\" id=\"DejaVuSerif-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 367.740625 321.118125 \nL 367.740625 22.318125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"367.740625\" xlink:href=\"#mc5363a1cd4\" y=\"321.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 8 -->\n      <g transform=\"translate(364.559375 335.716562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2981 1275 \nQ 2981 1775 2732 2051 \nQ 2484 2328 2034 2328 \nQ 1584 2328 1336 2051 \nQ 1088 1775 1088 1275 \nQ 1088 772 1336 495 \nQ 1584 219 2034 219 \nQ 2484 219 2732 495 \nQ 2981 772 2981 1275 \nz\nM 2853 3541 \nQ 2853 3966 2637 4203 \nQ 2422 4441 2034 4441 \nQ 1650 4441 1433 4203 \nQ 1216 3966 1216 3541 \nQ 1216 3113 1433 2875 \nQ 1650 2638 2034 2638 \nQ 2422 2638 2637 2875 \nQ 2853 3113 2853 3541 \nz\nM 2516 2484 \nQ 3047 2413 3344 2092 \nQ 3641 1772 3641 1275 \nQ 3641 619 3225 264 \nQ 2809 -91 2034 -91 \nQ 1263 -91 845 264 \nQ 428 619 428 1275 \nQ 428 1772 725 2092 \nQ 1022 2413 1556 2484 \nQ 1084 2569 832 2842 \nQ 581 3116 581 3541 \nQ 581 4103 968 4426 \nQ 1356 4750 2034 4750 \nQ 2713 4750 3100 4426 \nQ 3488 4103 3488 3541 \nQ 3488 3116 3236 2842 \nQ 2984 2569 2516 2484 \nz\n\" id=\"DejaVuSerif-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 455.438677 321.118125 \nL 455.438677 22.318125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"455.438677\" xlink:href=\"#mc5363a1cd4\" y=\"321.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10 -->\n      <g transform=\"translate(449.076177 335.716562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 909 0 \nL 909 331 \nL 1722 331 \nL 1722 4213 \nL 781 3603 \nL 781 4013 \nL 1919 4750 \nL 2350 4750 \nL 2350 331 \nL 3163 331 \nL 3163 0 \nL 909 0 \nz\n\" id=\"DejaVuSerif-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 219 \nQ 2513 219 2750 744 \nQ 2988 1269 2988 2328 \nQ 2988 3391 2750 3916 \nQ 2513 4441 2034 4441 \nQ 1556 4441 1318 3916 \nQ 1081 3391 1081 2328 \nQ 1081 1269 1318 744 \nQ 1556 219 2034 219 \nz\nM 2034 -91 \nQ 1275 -91 848 546 \nQ 422 1184 422 2328 \nQ 422 3475 848 4112 \nQ 1275 4750 2034 4750 \nQ 2797 4750 3222 4112 \nQ 3647 3475 3647 2328 \nQ 3647 1184 3222 546 \nQ 2797 -91 2034 -91 \nz\n\" id=\"DejaVuSerif-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 543.136729 321.118125 \nL 543.136729 22.318125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"543.136729\" xlink:href=\"#mc5363a1cd4\" y=\"321.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12 -->\n      <g transform=\"translate(536.774229 335.716562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 630.834781 321.118125 \nL 630.834781 22.318125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"630.834781\" xlink:href=\"#mc5363a1cd4\" y=\"321.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 14 -->\n      <g transform=\"translate(624.472281 335.716562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Epoch -->\n     <g transform=\"translate(351.859375 349.394687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 353 0 \nL 353 331 \nL 947 331 \nL 947 4331 \nL 353 4331 \nL 353 4666 \nL 4109 4666 \nL 4109 3628 \nL 3725 3628 \nL 3725 4281 \nL 1581 4281 \nL 1581 2719 \nL 3109 2719 \nL 3109 3303 \nL 3494 3303 \nL 3494 1753 \nL 3109 1753 \nL 3109 2338 \nL 1581 2338 \nL 1581 384 \nL 3775 384 \nL 3775 1038 \nL 4159 1038 \nL 4159 0 \nL 353 0 \nz\n\" id=\"DejaVuSerif-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1313 1825 \nL 1313 1497 \nQ 1313 897 1542 583 \nQ 1772 269 2209 269 \nQ 2650 269 2876 622 \nQ 3103 975 3103 1663 \nQ 3103 2353 2876 2703 \nQ 2650 3053 2209 3053 \nQ 1772 3053 1542 2737 \nQ 1313 2422 1313 1825 \nz\nM 738 2988 \nL 184 2988 \nL 184 3322 \nL 1313 3322 \nL 1313 2803 \nQ 1481 3116 1742 3264 \nQ 2003 3413 2388 3413 \nQ 3000 3413 3387 2928 \nQ 3775 2444 3775 1663 \nQ 3775 881 3387 395 \nQ 3000 -91 2388 -91 \nQ 2003 -91 1742 57 \nQ 1481 206 1313 519 \nL 1313 -997 \nL 1856 -997 \nL 1856 -1331 \nL 184 -1331 \nL 184 -997 \nL 738 -997 \nL 738 2988 \nz\n\" id=\"DejaVuSerif-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1925 219 \nQ 2388 219 2623 584 \nQ 2859 950 2859 1663 \nQ 2859 2375 2623 2739 \nQ 2388 3103 1925 3103 \nQ 1463 3103 1227 2739 \nQ 991 2375 991 1663 \nQ 991 950 1228 584 \nQ 1466 219 1925 219 \nz\nM 1925 -91 \nQ 1200 -91 759 389 \nQ 319 869 319 1663 \nQ 319 2456 758 2934 \nQ 1197 3413 1925 3413 \nQ 2653 3413 3092 2934 \nQ 3531 2456 3531 1663 \nQ 3531 869 3092 389 \nQ 2653 -91 1925 -91 \nz\n\" id=\"DejaVuSerif-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3291 997 \nQ 3169 466 2822 187 \nQ 2475 -91 1925 -91 \nQ 1200 -91 759 389 \nQ 319 869 319 1663 \nQ 319 2459 759 2936 \nQ 1200 3413 1925 3413 \nQ 2241 3413 2553 3339 \nQ 2866 3266 3181 3116 \nL 3181 2266 \nL 2847 2266 \nQ 2781 2703 2561 2903 \nQ 2341 3103 1931 3103 \nQ 1466 3103 1228 2742 \nQ 991 2381 991 1663 \nQ 991 944 1227 581 \nQ 1463 219 1931 219 \nQ 2303 219 2525 412 \nQ 2747 606 2828 997 \nL 3291 997 \nz\n\" id=\"DejaVuSerif-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 263 0 \nL 263 331 \nL 781 331 \nL 781 4531 \nL 231 4531 \nL 231 4863 \nL 1356 4863 \nL 1356 2731 \nQ 1516 3069 1770 3241 \nQ 2025 3413 2363 3413 \nQ 2913 3413 3172 3097 \nQ 3431 2781 3431 2113 \nL 3431 331 \nL 3944 331 \nL 3944 0 \nL 2356 0 \nL 2356 331 \nL 2853 331 \nL 2853 1931 \nQ 2853 2541 2704 2764 \nQ 2556 2988 2175 2988 \nQ 1775 2988 1565 2697 \nQ 1356 2406 1356 1850 \nL 1356 331 \nL 1856 331 \nL 1856 0 \nL 263 0 \nz\n\" id=\"DejaVuSerif-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-45\"/>\n      <use x=\"72.998047\" xlink:href=\"#DejaVuSerif-70\"/>\n      <use x=\"137.011719\" xlink:href=\"#DejaVuSerif-6f\"/>\n      <use x=\"197.216797\" xlink:href=\"#DejaVuSerif-63\"/>\n      <use x=\"253.222656\" xlink:href=\"#DejaVuSerif-68\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 286.829 \nL 705.378125 286.829 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5adbce6c9b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"286.829\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 290.628219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 603 325 \nQ 603 500 722 622 \nQ 841 744 1019 744 \nQ 1191 744 1312 622 \nQ 1434 500 1434 325 \nQ 1434 153 1312 31 \nQ 1191 -91 1019 -91 \nQ 841 -91 722 29 \nQ 603 150 603 325 \nz\n\" id=\"DejaVuSerif-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 249.837683 \nL 705.378125 249.837683 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"249.837683\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 253.636902)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 212.846366 \nL 705.378125 212.846366 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"212.846366\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.2 -->\n      <g transform=\"translate(7.2 216.645585)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 175.855049 \nL 705.378125 175.855049 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"175.855049\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.4 -->\n      <g transform=\"translate(7.2 179.654268)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 138.863732 \nL 705.378125 138.863732 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"138.863732\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.6 -->\n      <g transform=\"translate(7.2 142.662951)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 101.872415 \nL 705.378125 101.872415 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"101.872415\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.8 -->\n      <g transform=\"translate(7.2 105.671634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 64.881098 \nL 705.378125 64.881098 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"64.881098\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 68.680317)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 30.103125 27.889781 \nL 705.378125 27.889781 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"27.889781\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.2 -->\n      <g transform=\"translate(7.2 31.689)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 77.671638 \nL 104.646469 125.665387 \nL 148.495495 146.985409 \nL 192.344521 159.556338 \nL 236.193547 176.13665 \nL 280.042573 183.787901 \nL 323.891599 192.719055 \nL 367.740625 202.374894 \nL 411.589651 209.342863 \nL 455.438677 223.758276 \nL 499.287703 231.734382 \nL 543.136729 249.815881 \nL 586.985755 263.513225 \nL 630.834781 279.894816 \nL 674.683807 295.284901 \n\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 73.482365 \nL 104.646469 112.932655 \nL 148.495495 128.762468 \nL 192.344521 135.074953 \nL 236.193547 143.6611 \nL 280.042573 141.293624 \nL 323.891599 141.579606 \nL 367.740625 142.592575 \nL 411.589651 137.099835 \nL 455.438677 130.228832 \nL 499.287703 116.929004 \nL 543.136729 110.444492 \nL 586.985755 98.903491 \nL 630.834781 75.207379 \nL 674.683807 50.74562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 65.018519 \nL 104.646469 123.454046 \nL 148.495495 142.946019 \nL 192.344521 164.542542 \nL 236.193547 176.567614 \nL 280.042573 188.310865 \nL 323.891599 199.049216 \nL 367.740625 208.267155 \nL 411.589651 220.740209 \nL 455.438677 232.590723 \nL 499.287703 244.019536 \nL 543.136729 257.201731 \nL 586.985755 275.729946 \nL 630.834781 289.385892 \nL 674.683807 307.536307 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 62.571641 \nL 104.646469 113.063932 \nL 148.495495 123.934323 \nL 192.344521 139.808374 \nL 236.193547 144.448584 \nL 280.042573 140.710105 \nL 323.891599 140.929651 \nL 367.740625 134.00934 \nL 411.589651 132.16351 \nL 455.438677 118.454845 \nL 499.287703 114.755807 \nL 543.136729 98.086169 \nL 586.985755 86.342573 \nL 630.834781 62.644328 \nL 674.683807 35.899943 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 65.671527 \nL 104.646469 95.310456 \nL 148.495495 125.101721 \nL 192.344521 136.317124 \nL 236.193547 147.605572 \nL 280.042573 151.888217 \nL 323.891599 158.50514 \nL 367.740625 164.086404 \nL 411.589651 167.851737 \nL 455.438677 174.941247 \nL 499.287703 176.384567 \nL 543.136729 181.223985 \nL 586.985755 185.470453 \nL 630.834781 186.479214 \nL 674.683807 191.810865 \n\" style=\"fill:none;stroke:#2ca02c;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 63.882052 \nL 104.646469 88.950084 \nL 148.495495 115.02456 \nL 192.344521 124.413094 \nL 236.193547 132.219667 \nL 280.042573 134.71745 \nL 323.891599 138.838586 \nL 367.740625 141.442577 \nL 411.589651 142.647759 \nL 455.438677 147.886233 \nL 499.287703 148.354949 \nL 543.136729 149.933495 \nL 586.985755 150.646089 \nL 630.834781 149.350343 \nL 674.683807 149.451374 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 73.653273 \nL 104.646469 122.71191 \nL 148.495495 128.707454 \nL 192.344521 156.986523 \nL 236.193547 164.600051 \nL 280.042573 181.474234 \nL 323.891599 188.570465 \nL 367.740625 197.074069 \nL 411.589651 212.642156 \nL 455.438677 223.40974 \nL 499.287703 235.862516 \nL 543.136729 246.771271 \nL 586.985755 265.183561 \nL 630.834781 277.913603 \nL 674.683807 294.867322 \n\" style=\"fill:none;stroke:#d62728;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 70.653037 \nL 104.646469 112.797755 \nL 148.495495 116.904875 \nL 192.344521 136.03474 \nL 236.193547 138.557144 \nL 280.042573 145.415201 \nL 323.891599 144.60609 \nL 367.740625 139.174831 \nL 411.589651 139.632021 \nL 455.438677 132.617789 \nL 499.287703 124.128744 \nL 543.136729 108.758856 \nL 586.985755 95.203758 \nL 630.834781 78.851586 \nL 674.683807 64.898811 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 64.726788 \nL 104.646469 101.793586 \nL 148.495495 125.983159 \nL 192.344521 137.850921 \nL 236.193547 146.423357 \nL 280.042573 153.771842 \nL 323.891599 159.23978 \nL 367.740625 168.44056 \nL 411.589651 171.25402 \nL 455.438677 179.199979 \nL 499.287703 182.46287 \nL 543.136729 188.565366 \nL 586.985755 188.603776 \nL 630.834781 194.729187 \nL 674.683807 200.028061 \n\" style=\"fill:none;stroke:#9467bd;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 62.618964 \nL 104.646469 95.989564 \nL 148.495495 116.888283 \nL 192.344521 127.739901 \nL 236.193547 133.3059 \nL 280.042573 137.637654 \nL 323.891599 141.220237 \nL 367.740625 147.104352 \nL 411.589651 146.886812 \nL 455.438677 151.341051 \nL 499.287703 151.231733 \nL 543.136729 153.078629 \nL 586.985755 151.236684 \nL 630.834781 153.01164 \nL 674.683807 152.647103 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 84.326128 \nL 104.646469 124.722074 \nL 148.495495 150.239841 \nL 192.344521 161.407332 \nL 236.193547 172.338961 \nL 280.042573 184.103006 \nL 323.891599 194.726601 \nL 367.740625 207.505111 \nL 411.589651 217.388006 \nL 455.438677 224.033681 \nL 499.287703 236.27855 \nL 543.136729 255.047861 \nL 586.985755 268.579235 \nL 630.834781 278.990296 \nL 674.683807 297.329334 \n\" style=\"fill:none;stroke:#8c564b;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 80.883159 \nL 104.646469 113.955123 \nL 148.495495 131.759416 \nL 192.344521 136.4955 \nL 236.193547 138.687584 \nL 280.042573 142.989472 \nL 323.891599 139.639818 \nL 367.740625 134.138737 \nL 411.589651 134.657947 \nL 455.438677 125.9997 \nL 499.287703 112.918704 \nL 543.136729 103.935977 \nL 586.985755 85.57504 \nL 630.834781 73.690177 \nL 674.683807 55.354669 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 65.636097 \nL 104.646469 85.869588 \nL 148.495495 121.074057 \nL 192.344521 121.461674 \nL 236.193547 143.283612 \nL 280.042573 147.737213 \nL 323.891599 154.636903 \nL 367.740625 163.959103 \nL 411.589651 167.737895 \nL 455.438677 171.424943 \nL 499.287703 176.663645 \nL 543.136729 178.3568 \nL 586.985755 182.843541 \nL 630.834781 184.14186 \nL 674.683807 187.132676 \n\" style=\"fill:none;stroke:#e377c2;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p43e57b6c5b)\" d=\"M 60.797443 63.72648 \nL 104.646469 81.497883 \nL 148.495495 111.861967 \nL 192.344521 109.197207 \nL 236.193547 129.619586 \nL 280.042573 131.664541 \nL 323.891599 136.67836 \nL 367.740625 142.276606 \nL 411.589651 144.066104 \nL 455.438677 145.984328 \nL 499.287703 146.233275 \nL 543.136729 146.660909 \nL 586.985755 148.535304 \nL 630.834781 143.511448 \nL 674.683807 143.991862 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 321.118125 \nL 30.103125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 705.378125 321.118125 \nL 705.378125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 321.118125 \nL 705.378125 321.118125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 22.318125 \nL 705.378125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Cross-Entropy loss: solid — test, dashed — train -->\n    <g transform=\"translate(220.936562 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 4513 1234 \nQ 4306 581 3820 245 \nQ 3334 -91 2591 -91 \nQ 2134 -91 1743 65 \nQ 1353 222 1050 525 \nQ 700 875 529 1320 \nQ 359 1766 359 2328 \nQ 359 3416 987 4083 \nQ 1616 4750 2644 4750 \nQ 3025 4750 3456 4650 \nQ 3888 4550 4384 4347 \nL 4384 3272 \nL 4031 3272 \nQ 3916 3859 3567 4137 \nQ 3219 4416 2591 4416 \nQ 1844 4416 1459 3886 \nQ 1075 3356 1075 2328 \nQ 1075 1303 1459 773 \nQ 1844 244 2591 244 \nQ 3113 244 3450 492 \nQ 3788 741 3938 1234 \nL 4513 1234 \nz\n\" id=\"DejaVuSerif-43\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3059 3328 \nL 3059 2497 \nL 2728 2497 \nQ 2713 2744 2591 2866 \nQ 2469 2988 2234 2988 \nQ 1809 2988 1582 2694 \nQ 1356 2400 1356 1850 \nL 1356 331 \nL 2022 331 \nL 2022 0 \nL 263 0 \nL 263 331 \nL 781 331 \nL 781 2994 \nL 231 2994 \nL 231 3322 \nL 1356 3322 \nL 1356 2731 \nQ 1525 3078 1790 3245 \nQ 2056 3413 2438 3413 \nQ 2578 3413 2733 3391 \nQ 2888 3369 3059 3328 \nz\n\" id=\"DejaVuSerif-72\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 359 184 \nL 359 959 \nL 691 959 \nQ 703 588 923 403 \nQ 1144 219 1575 219 \nQ 1963 219 2166 364 \nQ 2369 509 2369 788 \nQ 2369 1006 2220 1140 \nQ 2072 1275 1594 1428 \nL 1178 1569 \nQ 750 1706 558 1912 \nQ 366 2119 366 2438 \nQ 366 2894 700 3153 \nQ 1034 3413 1625 3413 \nQ 1888 3413 2178 3344 \nQ 2469 3275 2778 3144 \nL 2778 2419 \nL 2447 2419 \nQ 2434 2741 2221 2922 \nQ 2009 3103 1644 3103 \nQ 1281 3103 1095 2975 \nQ 909 2847 909 2591 \nQ 909 2381 1050 2254 \nQ 1191 2128 1613 1997 \nL 2069 1856 \nQ 2541 1709 2748 1489 \nQ 2956 1269 2956 922 \nQ 2956 450 2595 179 \nQ 2234 -91 1600 -91 \nQ 1278 -91 972 -22 \nQ 666 47 359 184 \nz\n\" id=\"DejaVuSerif-73\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 281 1959 \nL 1881 1959 \nL 1881 1472 \nL 281 1472 \nL 281 1959 \nz\n\" id=\"DejaVuSerif-2d\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 263 0 \nL 263 331 \nL 781 331 \nL 781 2988 \nL 231 2988 \nL 231 3322 \nL 1356 3322 \nL 1356 2731 \nQ 1516 3069 1770 3241 \nQ 2025 3413 2363 3413 \nQ 2913 3413 3172 3097 \nQ 3431 2781 3431 2113 \nL 3431 331 \nL 3944 331 \nL 3944 0 \nL 2356 0 \nL 2356 331 \nL 2853 331 \nL 2853 1931 \nQ 2853 2541 2703 2767 \nQ 2553 2994 2175 2994 \nQ 1775 2994 1565 2701 \nQ 1356 2409 1356 1850 \nL 1356 331 \nL 1856 331 \nL 1856 0 \nL 263 0 \nz\n\" id=\"DejaVuSerif-6e\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 691 2988 \nL 184 2988 \nL 184 3322 \nL 691 3322 \nL 691 4353 \nL 1269 4353 \nL 1269 3322 \nL 2350 3322 \nL 2350 2988 \nL 1269 2988 \nL 1269 878 \nQ 1269 456 1350 337 \nQ 1431 219 1650 219 \nQ 1875 219 1978 351 \nQ 2081 484 2088 781 \nL 2522 781 \nQ 2497 328 2275 118 \nQ 2053 -91 1600 -91 \nQ 1103 -91 897 129 \nQ 691 350 691 878 \nL 691 2988 \nz\n\" id=\"DejaVuSerif-74\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 1381 -609 \nL 1600 -56 \nL 359 2988 \nL -19 2988 \nL -19 3322 \nL 1509 3322 \nL 1509 2988 \nL 978 2988 \nL 1913 703 \nL 2847 2988 \nL 2350 2988 \nL 2350 3322 \nL 3597 3322 \nL 3597 2988 \nL 3225 2988 \nL 1703 -750 \nQ 1547 -1138 1356 -1280 \nQ 1166 -1422 819 -1422 \nQ 672 -1422 517 -1397 \nQ 363 -1372 206 -1325 \nL 206 -691 \nL 500 -691 \nQ 519 -903 608 -995 \nQ 697 -1088 884 -1088 \nQ 1056 -1088 1161 -992 \nQ 1266 -897 1381 -609 \nz\n\" id=\"DejaVuSerif-79\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSerif-20\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 1313 331 \nL 1856 331 \nL 1856 0 \nL 184 0 \nL 184 331 \nL 738 331 \nL 738 4531 \nL 184 4531 \nL 184 4863 \nL 1313 4863 \nL 1313 331 \nz\n\" id=\"DejaVuSerif-6c\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 666 325 \nQ 666 500 786 622 \nQ 906 744 1081 744 \nQ 1256 744 1376 622 \nQ 1497 500 1497 325 \nQ 1497 150 1378 29 \nQ 1259 -91 1081 -91 \nQ 903 -91 784 29 \nQ 666 150 666 325 \nz\nM 666 2363 \nQ 666 2538 786 2658 \nQ 906 2778 1081 2778 \nQ 1259 2778 1378 2659 \nQ 1497 2541 1497 2363 \nQ 1497 2184 1378 2065 \nQ 1259 1947 1081 1947 \nQ 906 1947 786 2067 \nQ 666 2188 666 2363 \nz\n\" id=\"DejaVuSerif-3a\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 622 4353 \nQ 622 4497 726 4603 \nQ 831 4709 978 4709 \nQ 1122 4709 1226 4603 \nQ 1331 4497 1331 4353 \nQ 1331 4206 1228 4103 \nQ 1125 4000 978 4000 \nQ 831 4000 726 4103 \nQ 622 4206 622 4353 \nz\nM 1356 331 \nL 1900 331 \nL 1900 0 \nL 231 0 \nL 231 331 \nL 781 331 \nL 781 2988 \nL 231 2988 \nL 231 3322 \nL 1356 3322 \nL 1356 331 \nz\n\" id=\"DejaVuSerif-69\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3359 331 \nL 3909 331 \nL 3909 0 \nL 2784 0 \nL 2784 519 \nQ 2616 206 2355 57 \nQ 2094 -91 1709 -91 \nQ 1097 -91 708 395 \nQ 319 881 319 1663 \nQ 319 2444 706 2928 \nQ 1094 3413 1709 3413 \nQ 2094 3413 2355 3264 \nQ 2616 3116 2784 2803 \nL 2784 4531 \nL 2241 4531 \nL 2241 4863 \nL 3359 4863 \nL 3359 331 \nz\nM 2784 1497 \nL 2784 1825 \nQ 2784 2422 2554 2737 \nQ 2325 3053 1888 3053 \nQ 1444 3053 1217 2703 \nQ 991 2353 991 1663 \nQ 991 975 1217 622 \nQ 1444 269 1888 269 \nQ 2325 269 2554 583 \nQ 2784 897 2784 1497 \nz\n\" id=\"DejaVuSerif-64\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 281 1906 \nL 6119 1906 \nL 6119 1491 \nL 281 1491 \nL 281 1906 \nz\n\" id=\"DejaVuSerif-2014\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3469 1600 \nL 991 1600 \nL 991 1575 \nQ 991 903 1244 561 \nQ 1497 219 1991 219 \nQ 2369 219 2611 417 \nQ 2853 616 2950 1006 \nL 3413 1006 \nQ 3275 459 2904 184 \nQ 2534 -91 1931 -91 \nQ 1203 -91 761 389 \nQ 319 869 319 1663 \nQ 319 2450 753 2931 \nQ 1188 3413 1894 3413 \nQ 2647 3413 3050 2948 \nQ 3453 2484 3469 1600 \nz\nM 2791 1931 \nQ 2772 2513 2545 2808 \nQ 2319 3103 1894 3103 \nQ 1497 3103 1269 2806 \nQ 1041 2509 991 1931 \nL 2791 1931 \nz\n\" id=\"DejaVuSerif-65\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 231 -622 \nQ 525 -406 662 -114 \nQ 800 178 800 594 \nL 800 709 \nL 1416 709 \nQ 1391 175 1164 -208 \nQ 938 -591 481 -872 \nL 231 -622 \nz\n\" id=\"DejaVuSerif-2c\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2547 1044 \nL 2547 1747 \nL 1806 1747 \nQ 1378 1747 1168 1562 \nQ 959 1378 959 997 \nQ 959 650 1171 447 \nQ 1384 244 1747 244 \nQ 2106 244 2326 466 \nQ 2547 688 2547 1044 \nz\nM 3122 2075 \nL 3122 331 \nL 3634 331 \nL 3634 0 \nL 2547 0 \nL 2547 359 \nQ 2356 128 2106 18 \nQ 1856 -91 1522 -91 \nQ 969 -91 644 203 \nQ 319 497 319 997 \nQ 319 1513 691 1797 \nQ 1063 2081 1741 2081 \nL 2547 2081 \nL 2547 2309 \nQ 2547 2688 2317 2895 \nQ 2088 3103 1672 3103 \nQ 1328 3103 1125 2947 \nQ 922 2791 872 2484 \nL 575 2484 \nL 575 3156 \nQ 875 3284 1158 3348 \nQ 1441 3413 1709 3413 \nQ 2400 3413 2761 3070 \nQ 3122 2728 3122 2075 \nz\n\" id=\"DejaVuSerif-61\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSerif-43\"/>\n     <use x=\"76.513672\" xlink:href=\"#DejaVuSerif-72\"/>\n     <use x=\"124.316406\" xlink:href=\"#DejaVuSerif-6f\"/>\n     <use x=\"184.521484\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"235.839844\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"287.158203\" xlink:href=\"#DejaVuSerif-2d\"/>\n     <use x=\"320.947266\" xlink:href=\"#DejaVuSerif-45\"/>\n     <use x=\"393.945312\" xlink:href=\"#DejaVuSerif-6e\"/>\n     <use x=\"458.349609\" xlink:href=\"#DejaVuSerif-74\"/>\n     <use x=\"498.535156\" xlink:href=\"#DejaVuSerif-72\"/>\n     <use x=\"546.337891\" xlink:href=\"#DejaVuSerif-6f\"/>\n     <use x=\"606.542969\" xlink:href=\"#DejaVuSerif-70\"/>\n     <use x=\"670.556641\" xlink:href=\"#DejaVuSerif-79\"/>\n     <use x=\"727.050781\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"758.837891\" xlink:href=\"#DejaVuSerif-6c\"/>\n     <use x=\"790.820312\" xlink:href=\"#DejaVuSerif-6f\"/>\n     <use x=\"851.025391\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"902.34375\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"953.662109\" xlink:href=\"#DejaVuSerif-3a\"/>\n     <use x=\"987.353516\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"1019.140625\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"1070.458984\" xlink:href=\"#DejaVuSerif-6f\"/>\n     <use x=\"1130.664062\" xlink:href=\"#DejaVuSerif-6c\"/>\n     <use x=\"1162.646484\" xlink:href=\"#DejaVuSerif-69\"/>\n     <use x=\"1194.628906\" xlink:href=\"#DejaVuSerif-64\"/>\n     <use x=\"1258.642578\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"1290.429688\" xlink:href=\"#DejaVuSerif-2014\"/>\n     <use x=\"1390.429688\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"1422.216797\" xlink:href=\"#DejaVuSerif-74\"/>\n     <use x=\"1462.402344\" xlink:href=\"#DejaVuSerif-65\"/>\n     <use x=\"1521.582031\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"1572.900391\" xlink:href=\"#DejaVuSerif-74\"/>\n     <use x=\"1613.085938\" xlink:href=\"#DejaVuSerif-2c\"/>\n     <use x=\"1644.873047\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"1676.660156\" xlink:href=\"#DejaVuSerif-64\"/>\n     <use x=\"1740.673828\" xlink:href=\"#DejaVuSerif-61\"/>\n     <use x=\"1800.292969\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"1851.611328\" xlink:href=\"#DejaVuSerif-68\"/>\n     <use x=\"1916.015625\" xlink:href=\"#DejaVuSerif-65\"/>\n     <use x=\"1975.195312\" xlink:href=\"#DejaVuSerif-64\"/>\n     <use x=\"2039.208984\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"2070.996094\" xlink:href=\"#DejaVuSerif-2014\"/>\n     <use x=\"2170.996094\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"2202.783203\" xlink:href=\"#DejaVuSerif-74\"/>\n     <use x=\"2242.96875\" xlink:href=\"#DejaVuSerif-72\"/>\n     <use x=\"2290.771484\" xlink:href=\"#DejaVuSerif-61\"/>\n     <use x=\"2350.390625\" xlink:href=\"#DejaVuSerif-69\"/>\n     <use x=\"2382.373047\" xlink:href=\"#DejaVuSerif-6e\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 37.103125 316.118125 \nL 204.79375 316.118125 \nQ 206.79375 316.118125 206.79375 314.118125 \nL 206.79375 211.8025 \nQ 206.79375 209.8025 204.79375 209.8025 \nL 37.103125 209.8025 \nQ 35.103125 209.8025 35.103125 211.8025 \nL 35.103125 314.118125 \nQ 35.103125 316.118125 37.103125 316.118125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_45\">\n     <path d=\"M 39.103125 217.900937 \nL 59.103125 217.900937 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_46\"/>\n    <g id=\"text_18\">\n     <!-- torch.nn.LSTM -->\n     <g transform=\"translate(67.103125 221.400937)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 353 0 \nL 353 331 \nL 947 331 \nL 947 4331 \nL 353 4331 \nL 353 4666 \nL 2175 4666 \nL 2175 4331 \nL 1581 4331 \nL 1581 384 \nL 3713 384 \nL 3713 1166 \nL 4097 1166 \nL 4097 0 \nL 353 0 \nz\n\" id=\"DejaVuSerif-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 594 225 \nL 594 1288 \nL 953 1284 \nQ 969 753 1261 498 \nQ 1553 244 2150 244 \nQ 2706 244 2998 464 \nQ 3291 684 3291 1106 \nQ 3291 1444 3114 1625 \nQ 2938 1806 2369 1978 \nL 1753 2163 \nQ 1084 2366 811 2669 \nQ 538 2972 538 3500 \nQ 538 4094 959 4422 \nQ 1381 4750 2144 4750 \nQ 2469 4750 2856 4679 \nQ 3244 4609 3681 4475 \nL 3681 3481 \nL 3328 3481 \nQ 3275 3975 2998 4195 \nQ 2722 4416 2156 4416 \nQ 1663 4416 1405 4214 \nQ 1147 4013 1147 3628 \nQ 1147 3294 1340 3103 \nQ 1534 2913 2163 2725 \nL 2741 2553 \nQ 3375 2363 3645 2067 \nQ 3916 1772 3916 1275 \nQ 3916 597 3481 253 \nQ 3047 -91 2188 -91 \nQ 1803 -91 1404 -12 \nQ 1006 66 594 225 \nz\n\" id=\"DejaVuSerif-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1222 0 \nL 1222 331 \nL 1819 331 \nL 1819 4294 \nL 447 4294 \nL 447 3566 \nL 63 3566 \nL 63 4666 \nL 4206 4666 \nL 4206 3566 \nL 3822 3566 \nL 3822 4294 \nL 2450 4294 \nL 2450 331 \nL 3047 331 \nL 3047 0 \nL 1222 0 \nz\n\" id=\"DejaVuSerif-54\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 353 0 \nL 353 331 \nL 947 331 \nL 947 4331 \nL 319 4331 \nL 319 4666 \nL 1678 4666 \nL 3316 1344 \nL 4953 4666 \nL 6228 4666 \nL 6228 4331 \nL 5606 4331 \nL 5606 331 \nL 6203 331 \nL 6203 0 \nL 4378 0 \nL 4378 331 \nL 4972 331 \nL 4972 3938 \nL 3372 684 \nL 2931 684 \nL 1331 3938 \nL 1331 331 \nL 1925 331 \nL 1925 0 \nL 353 0 \nz\n\" id=\"DejaVuSerif-4d\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-74\"/>\n      <use x=\"40.185547\" xlink:href=\"#DejaVuSerif-6f\"/>\n      <use x=\"100.390625\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"148.193359\" xlink:href=\"#DejaVuSerif-63\"/>\n      <use x=\"204.199219\" xlink:href=\"#DejaVuSerif-68\"/>\n      <use x=\"268.603516\" xlink:href=\"#DejaVuSerif-2e\"/>\n      <use x=\"300.390625\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"364.794922\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"429.199219\" xlink:href=\"#DejaVuSerif-2e\"/>\n      <use x=\"460.986328\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"527.392578\" xlink:href=\"#DejaVuSerif-53\"/>\n      <use x=\"595.898438\" xlink:href=\"#DejaVuSerif-54\"/>\n      <use x=\"662.597656\" xlink:href=\"#DejaVuSerif-4d\"/>\n     </g>\n    </g>\n    <g id=\"line2d_47\">\n     <path d=\"M 39.103125 232.579062 \nL 59.103125 232.579062 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_48\"/>\n    <g id=\"text_19\">\n     <!-- RNNLayer -->\n     <g transform=\"translate(67.103125 236.079062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3066 2316 \nQ 3284 2256 3442 2114 \nQ 3600 1972 3725 1716 \nL 4403 331 \nL 4972 331 \nL 4972 0 \nL 3872 0 \nL 3144 1484 \nQ 2934 1916 2759 2042 \nQ 2584 2169 2278 2169 \nL 1581 2169 \nL 1581 331 \nL 2241 331 \nL 2241 0 \nL 353 0 \nL 353 331 \nL 947 331 \nL 947 4331 \nL 353 4331 \nL 353 4666 \nL 2719 4666 \nQ 3400 4666 3770 4341 \nQ 4141 4016 4141 3419 \nQ 4141 2938 3870 2661 \nQ 3600 2384 3066 2316 \nz\nM 1581 2503 \nL 2503 2503 \nQ 2975 2503 3200 2726 \nQ 3425 2950 3425 3419 \nQ 3425 3888 3200 4109 \nQ 2975 4331 2503 4331 \nL 1581 4331 \nL 1581 2503 \nz\n\" id=\"DejaVuSerif-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 0 \nL 313 331 \nL 941 331 \nL 941 4331 \nL 313 4331 \nL 313 4666 \nL 1509 4666 \nL 4306 984 \nL 4306 4331 \nL 3681 4331 \nL 3681 4666 \nL 5319 4666 \nL 5319 4331 \nL 4691 4331 \nL 4691 -91 \nL 4313 -91 \nL 1325 3841 \nL 1325 331 \nL 1953 331 \nL 1953 0 \nL 313 0 \nz\n\" id=\"DejaVuSerif-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"75.292969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"250.292969\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"316.699219\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"376.318359\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"432.8125\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"491.992188\" xlink:href=\"#DejaVuSerif-72\"/>\n     </g>\n    </g>\n    <g id=\"line2d_49\">\n     <path d=\"M 39.103125 247.399375 \nL 59.103125 247.399375 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_50\"/>\n    <g id=\"text_20\">\n     <!-- RNNLayer + VarDO -->\n     <g transform=\"translate(67.103125 250.899375)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2931 4013 \nL 2931 2259 \nL 4684 2259 \nL 4684 1753 \nL 2931 1753 \nL 2931 0 \nL 2431 0 \nL 2431 1753 \nL 678 1753 \nL 678 2259 \nL 2431 2259 \nL 2431 4013 \nL 2931 4013 \nz\n\" id=\"DejaVuSerif-2b\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1119 4331 \nL 2497 750 \nL 3872 4331 \nL 3347 4331 \nL 3347 4666 \nL 4716 4666 \nL 4716 4331 \nL 4263 4331 \nL 2597 0 \nL 2059 0 \nL 403 4331 \nL -63 4331 \nL -63 4666 \nL 1638 4666 \nL 1638 4331 \nL 1119 4331 \nz\n\" id=\"DejaVuSerif-56\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1581 331 \nL 2163 331 \nQ 3072 331 3558 850 \nQ 4044 1369 4044 2338 \nQ 4044 3306 3559 3818 \nQ 3075 4331 2163 4331 \nL 1581 4331 \nL 1581 331 \nz\nM 353 0 \nL 353 331 \nL 947 331 \nL 947 4331 \nL 353 4331 \nL 353 4666 \nL 2209 4666 \nQ 3416 4666 4089 4050 \nQ 4763 3434 4763 2338 \nQ 4763 1238 4088 619 \nQ 3413 0 2209 0 \nL 353 0 \nz\n\" id=\"DejaVuSerif-44\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2625 244 \nQ 3391 244 3781 770 \nQ 4172 1297 4172 2328 \nQ 4172 3363 3781 3889 \nQ 3391 4416 2625 4416 \nQ 1856 4416 1465 3889 \nQ 1075 3363 1075 2328 \nQ 1075 1297 1465 770 \nQ 1856 244 2625 244 \nz\nM 2625 -91 \nQ 2150 -91 1751 65 \nQ 1353 222 1050 525 \nQ 700 875 529 1319 \nQ 359 1763 359 2328 \nQ 359 2894 529 3339 \nQ 700 3784 1050 4134 \nQ 1356 4441 1750 4595 \nQ 2144 4750 2625 4750 \nQ 3641 4750 4266 4084 \nQ 4891 3419 4891 2328 \nQ 4891 1769 4719 1320 \nQ 4547 872 4197 525 \nQ 3891 219 3497 64 \nQ 3103 -91 2625 -91 \nz\n\" id=\"DejaVuSerif-4f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"75.292969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"250.292969\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"316.699219\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"376.318359\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"432.8125\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"491.992188\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"539.794922\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"571.582031\" xlink:href=\"#DejaVuSerif-2b\"/>\n      <use x=\"655.371094\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"687.158203\" xlink:href=\"#DejaVuSerif-56\"/>\n      <use x=\"750.25\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"809.869141\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"857.671875\" xlink:href=\"#DejaVuSerif-44\"/>\n      <use x=\"937.847656\" xlink:href=\"#DejaVuSerif-4f\"/>\n     </g>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 39.103125 262.219687 \nL 59.103125 262.219687 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_21\">\n     <!-- FastRNNLayer -->\n     <g transform=\"translate(67.103125 265.719687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 353 0 \nL 353 331 \nL 947 331 \nL 947 4331 \nL 353 4331 \nL 353 4666 \nL 4172 4666 \nL 4172 3628 \nL 3788 3628 \nL 3788 4281 \nL 1581 4281 \nL 1581 2719 \nL 3175 2719 \nL 3175 3303 \nL 3559 3303 \nL 3559 1753 \nL 3175 1753 \nL 3175 2338 \nL 1581 2338 \nL 1581 331 \nL 2328 331 \nL 2328 0 \nL 353 0 \nz\n\" id=\"DejaVuSerif-46\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-46\"/>\n      <use x=\"62.634766\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"122.253906\" xlink:href=\"#DejaVuSerif-73\"/>\n      <use x=\"173.572266\" xlink:href=\"#DejaVuSerif-74\"/>\n      <use x=\"213.757812\" xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"289.050781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"376.550781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"464.050781\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"530.457031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"590.076172\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"646.570312\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"705.75\" xlink:href=\"#DejaVuSerif-72\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 39.103125 277.04 \nL 59.103125 277.04 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_22\">\n     <!-- FastRNNLayer + VarDO -->\n     <g transform=\"translate(67.103125 280.54)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-46\"/>\n      <use x=\"62.634766\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"122.253906\" xlink:href=\"#DejaVuSerif-73\"/>\n      <use x=\"173.572266\" xlink:href=\"#DejaVuSerif-74\"/>\n      <use x=\"213.757812\" xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"289.050781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"376.550781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"464.050781\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"530.457031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"590.076172\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"646.570312\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"705.75\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"753.552734\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"785.339844\" xlink:href=\"#DejaVuSerif-2b\"/>\n      <use x=\"869.128906\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"900.916016\" xlink:href=\"#DejaVuSerif-56\"/>\n      <use x=\"964.007812\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"1023.626953\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"1071.429688\" xlink:href=\"#DejaVuSerif-44\"/>\n      <use x=\"1151.605469\" xlink:href=\"#DejaVuSerif-4f\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 39.103125 291.860312 \nL 59.103125 291.860312 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_23\">\n     <!-- HandmadeLSTM -->\n     <g transform=\"translate(67.103125 295.360312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 353 0 \nL 353 331 \nL 947 331 \nL 947 4331 \nL 353 4331 \nL 353 4666 \nL 2175 4666 \nL 2175 4331 \nL 1581 4331 \nL 1581 2719 \nL 4000 2719 \nL 4000 4331 \nL 3406 4331 \nL 3406 4666 \nL 5228 4666 \nL 5228 4331 \nL 4634 4331 \nL 4634 331 \nL 5228 331 \nL 5228 0 \nL 3406 0 \nL 3406 331 \nL 4000 331 \nL 4000 2338 \nL 1581 2338 \nL 1581 331 \nL 2175 331 \nL 2175 0 \nL 353 0 \nz\n\" id=\"DejaVuSerif-48\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3316 2675 \nQ 3481 3041 3739 3227 \nQ 3997 3413 4341 3413 \nQ 4863 3413 5119 3089 \nQ 5375 2766 5375 2113 \nL 5375 331 \nL 5894 331 \nL 5894 0 \nL 4300 0 \nL 4300 331 \nL 4800 331 \nL 4800 2047 \nQ 4800 2556 4650 2772 \nQ 4500 2988 4153 2988 \nQ 3769 2988 3567 2697 \nQ 3366 2406 3366 1850 \nL 3366 331 \nL 3866 331 \nL 3866 0 \nL 2291 0 \nL 2291 331 \nL 2791 331 \nL 2791 2069 \nQ 2791 2566 2641 2777 \nQ 2491 2988 2144 2988 \nQ 1759 2988 1557 2697 \nQ 1356 2406 1356 1850 \nL 1356 331 \nL 1856 331 \nL 1856 0 \nL 263 0 \nL 263 331 \nL 781 331 \nL 781 2994 \nL 231 2994 \nL 231 3322 \nL 1356 3322 \nL 1356 2731 \nQ 1516 3063 1762 3238 \nQ 2009 3413 2322 3413 \nQ 2709 3413 2968 3220 \nQ 3228 3028 3316 2675 \nz\n\" id=\"DejaVuSerif-6d\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSerif-48\"/>\n      <use x=\"87.207031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"146.826172\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"211.230469\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"275.244141\" xlink:href=\"#DejaVuSerif-6d\"/>\n      <use x=\"370.068359\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"429.6875\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"493.701172\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"552.880859\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"619.287109\" xlink:href=\"#DejaVuSerif-53\"/>\n      <use x=\"687.792969\" xlink:href=\"#DejaVuSerif-54\"/>\n      <use x=\"754.492188\" xlink:href=\"#DejaVuSerif-4d\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 39.103125 306.538437 \nL 59.103125 306.538437 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_24\">\n     <!-- HandmadeLSTM + RecDO -->\n     <g transform=\"translate(67.103125 310.038437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-48\"/>\n      <use x=\"87.207031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"146.826172\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"211.230469\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"275.244141\" xlink:href=\"#DejaVuSerif-6d\"/>\n      <use x=\"370.068359\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"429.6875\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"493.701172\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"552.880859\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"619.287109\" xlink:href=\"#DejaVuSerif-53\"/>\n      <use x=\"687.792969\" xlink:href=\"#DejaVuSerif-54\"/>\n      <use x=\"754.492188\" xlink:href=\"#DejaVuSerif-4d\"/>\n      <use x=\"856.884766\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"888.671875\" xlink:href=\"#DejaVuSerif-2b\"/>\n      <use x=\"972.460938\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"1004.248047\" xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"1079.541016\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"1138.720703\" xlink:href=\"#DejaVuSerif-63\"/>\n      <use x=\"1194.726562\" xlink:href=\"#DejaVuSerif-44\"/>\n      <use x=\"1274.902344\" xlink:href=\"#DejaVuSerif-4f\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 30.103125 675.718125 \nL 705.378125 675.718125 \nL 705.378125 376.918125 \nL 30.103125 376.918125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_8\">\n     <g id=\"line2d_59\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 104.646469 675.718125 \nL 104.646469 376.918125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.646469\" xlink:href=\"#mc5363a1cd4\" y=\"675.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 2 -->\n      <g transform=\"translate(101.465219 690.316562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_61\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 192.344521 675.718125 \nL 192.344521 376.918125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.344521\" xlink:href=\"#mc5363a1cd4\" y=\"675.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 4 -->\n      <g transform=\"translate(189.163271 690.316562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_63\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 280.042573 675.718125 \nL 280.042573 376.918125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"280.042573\" xlink:href=\"#mc5363a1cd4\" y=\"675.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 6 -->\n      <g transform=\"translate(276.861323 690.316562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_65\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 367.740625 675.718125 \nL 367.740625 376.918125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_66\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"367.740625\" xlink:href=\"#mc5363a1cd4\" y=\"675.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 8 -->\n      <g transform=\"translate(364.559375 690.316562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_67\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 455.438677 675.718125 \nL 455.438677 376.918125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_68\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"455.438677\" xlink:href=\"#mc5363a1cd4\" y=\"675.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 10 -->\n      <g transform=\"translate(449.076177 690.316562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_69\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 543.136729 675.718125 \nL 543.136729 376.918125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_70\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"543.136729\" xlink:href=\"#mc5363a1cd4\" y=\"675.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 12 -->\n      <g transform=\"translate(536.774229 690.316562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_71\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 630.834781 675.718125 \nL 630.834781 376.918125 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_72\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"630.834781\" xlink:href=\"#mc5363a1cd4\" y=\"675.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 14 -->\n      <g transform=\"translate(624.472281 690.316562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_32\">\n     <!-- Epoch -->\n     <g transform=\"translate(351.859375 703.994687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-45\"/>\n      <use x=\"72.998047\" xlink:href=\"#DejaVuSerif-70\"/>\n      <use x=\"137.011719\" xlink:href=\"#DejaVuSerif-6f\"/>\n      <use x=\"197.216797\" xlink:href=\"#DejaVuSerif-63\"/>\n      <use x=\"253.222656\" xlink:href=\"#DejaVuSerif-68\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_9\">\n     <g id=\"line2d_73\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 30.103125 626.135565 \nL 705.378125 626.135565 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_74\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"626.135565\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 0.3 -->\n      <g transform=\"translate(7.2 629.934784)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 622 4469 \nQ 988 4606 1323 4678 \nQ 1659 4750 1953 4750 \nQ 2638 4750 3022 4454 \nQ 3406 4159 3406 3634 \nQ 3406 3213 3140 2930 \nQ 2875 2647 2388 2547 \nQ 2963 2466 3280 2130 \nQ 3597 1794 3597 1259 \nQ 3597 606 3158 257 \nQ 2719 -91 1894 -91 \nQ 1528 -91 1179 -12 \nQ 831 66 488 225 \nL 488 1131 \nL 838 1131 \nQ 869 681 1141 450 \nQ 1413 219 1906 219 \nQ 2384 219 2661 495 \nQ 2938 772 2938 1253 \nQ 2938 1803 2653 2086 \nQ 2369 2369 1819 2369 \nL 1522 2369 \nL 1522 2688 \nL 1678 2688 \nQ 2225 2688 2498 2914 \nQ 2772 3141 2772 3597 \nQ 2772 4006 2547 4223 \nQ 2322 4441 1900 4441 \nQ 1478 4441 1245 4241 \nQ 1013 4041 972 3647 \nL 622 3647 \nL 622 4469 \nz\n\" id=\"DejaVuSerif-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_75\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 30.103125 574.852458 \nL 705.378125 574.852458 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_76\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"574.852458\"/>\n      </g>\n     </g>\n     <g id=\"text_34\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 578.651677)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_77\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 30.103125 523.56935 \nL 705.378125 523.56935 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_78\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"523.56935\"/>\n      </g>\n     </g>\n     <g id=\"text_35\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 527.368569)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 3219 4666 \nL 3219 4153 \nL 1081 4153 \nL 1081 2816 \nQ 1244 2928 1461 2984 \nQ 1678 3041 1947 3041 \nQ 2703 3041 3140 2622 \nQ 3578 2203 3578 1478 \nQ 3578 738 3136 323 \nQ 2694 -91 1894 -91 \nQ 1572 -91 1234 -12 \nQ 897 66 544 225 \nL 544 1131 \nL 897 1131 \nQ 925 688 1179 453 \nQ 1434 219 1894 219 \nQ 2388 219 2653 544 \nQ 2919 869 2919 1478 \nQ 2919 2084 2655 2407 \nQ 2391 2731 1894 2731 \nQ 1613 2731 1398 2631 \nQ 1184 2531 1019 2322 \nL 750 2322 \nL 750 4666 \nL 3219 4666 \nz\n\" id=\"DejaVuSerif-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_79\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 30.103125 472.286243 \nL 705.378125 472.286243 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_80\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"472.286243\"/>\n      </g>\n     </g>\n     <g id=\"text_36\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 476.085462)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSerif-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_81\">\n      <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 30.103125 421.003136 \nL 705.378125 421.003136 \n\" style=\"fill:none;stroke:#808080;stroke-dasharray:0.8,1.32;stroke-dashoffset:0;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_82\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m5adbce6c9b\" y=\"421.003136\"/>\n      </g>\n     </g>\n     <g id=\"text_37\">\n      <!-- 0.7 -->\n      <g transform=\"translate(7.2 424.802354)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 3609 4347 \nL 1784 0 \nL 1319 0 \nL 3059 4153 \nL 903 4153 \nL 903 3578 \nL 538 3578 \nL 538 4666 \nL 3609 4666 \nL 3609 4347 \nz\n\" id=\"DejaVuSerif-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSerif-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSerif-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSerif-37\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_83\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 633.294687 \nL 104.646469 594.832357 \nL 148.495495 578.667921 \nL 192.344521 566.052277 \nL 236.193547 549.641682 \nL 280.042573 542.236402 \nL 323.891599 532.143886 \nL 367.740625 520.861602 \nL 411.589651 513.948639 \nL 455.438677 494.379006 \nL 499.287703 487.876308 \nL 543.136729 461.721923 \nL 586.985755 445.126709 \nL 630.834781 423.710884 \nL 674.683807 405.966928 \n\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 640.20765 \nL 104.646469 605.601809 \nL 148.495495 594.381065 \nL 192.344521 585.74499 \nL 236.193547 580.124361 \nL 280.042573 578.688434 \nL 323.891599 580.944891 \nL 367.740625 578.770487 \nL 411.589651 578.360222 \nL 455.438677 579.262805 \nL 499.287703 586.257821 \nL 543.136729 587.160404 \nL 586.985755 588.001447 \nL 630.834781 592.760519 \nL 674.683807 594.770817 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 650.361705 \nL 104.646469 595.734939 \nL 148.495495 581.642341 \nL 192.344521 560.472675 \nL 236.193547 548.144216 \nL 280.042573 536.431154 \nL 323.891599 525.90786 \nL 367.740625 516.123043 \nL 411.589651 501.845826 \nL 455.438677 485.004454 \nL 499.287703 469.660548 \nL 543.136729 455.321791 \nL 586.985755 428.572522 \nL 630.834781 417.536397 \nL 674.683807 390.499943 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 660.146522 \nL 104.646469 606.791577 \nL 148.495495 593.9708 \nL 192.344521 583.755205 \nL 236.193547 580.206414 \nL 280.042573 578.873054 \nL 323.891599 575.611448 \nL 367.740625 580.021795 \nL 411.589651 579.714096 \nL 455.438677 586.052689 \nL 499.287703 588.124526 \nL 543.136729 589.129675 \nL 586.985755 592.534873 \nL 630.834781 590.319443 \nL 674.683807 596.863168 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 644.925696 \nL 104.646469 612.719904 \nL 148.495495 594.688764 \nL 192.344521 586.996298 \nL 236.193547 582.114146 \nL 280.042573 575.529395 \nL 323.891599 568.883104 \nL 367.740625 566.380489 \nL 411.589651 558.031599 \nL 455.438677 551.980192 \nL 499.287703 552.739182 \nL 543.136729 544.451832 \nL 586.985755 540.287643 \nL 630.834781 540.759448 \nL 674.683807 535.610624 \n\" style=\"fill:none;stroke:#2ca02c;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 651.65404 \nL 104.646469 622.484208 \nL 148.495495 603.488945 \nL 192.344521 598.565767 \nL 236.193547 595.037489 \nL 280.042573 591.529724 \nL 323.891599 588.247606 \nL 367.740625 584.124444 \nL 411.589651 581.047457 \nL 455.438677 578.708948 \nL 499.287703 577.642259 \nL 543.136729 575.324262 \nL 586.985755 574.975537 \nL 630.834781 574.216547 \nL 674.683807 573.806283 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 642.341027 \nL 104.646469 597.560618 \nL 148.495495 590.914327 \nL 192.344521 567.385637 \nL 236.193547 559.918817 \nL 280.042573 547.939083 \nL 323.891599 538.400425 \nL 367.740625 528.041237 \nL 411.589651 512.410146 \nL 455.438677 499.302184 \nL 499.287703 484.594189 \nL 543.136729 471.793925 \nL 586.985755 445.495948 \nL 630.834781 431.813615 \nL 674.683807 407.525935 \n\" style=\"fill:none;stroke:#d62728;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 648.453974 \nL 104.646469 607.7557 \nL 148.495495 603.427405 \nL 192.344521 589.068135 \nL 236.193547 583.796232 \nL 280.042573 577.929444 \nL 323.891599 578.544842 \nL 367.740625 581.867987 \nL 411.589651 579.303832 \nL 455.438677 579.632044 \nL 499.287703 581.396182 \nL 543.136729 583.324427 \nL 586.985755 589.929692 \nL 630.834781 590.524576 \nL 674.683807 595.058002 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 657.766986 \nL 104.646469 605.930021 \nL 148.495495 595.632373 \nL 192.344521 586.483467 \nL 236.193547 579.632044 \nL 280.042573 575.611448 \nL 323.891599 569.683121 \nL 367.740625 558.503403 \nL 411.589651 556.000788 \nL 455.438677 547.754464 \nL 499.287703 543.590276 \nL 543.136729 536.861932 \nL 586.985755 535.71319 \nL 630.834781 531.34387 \nL 674.683807 525.723241 \n\" style=\"fill:none;stroke:#9467bd;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 662.136307 \nL 104.646469 614.566096 \nL 148.495495 602.545336 \nL 192.344521 595.734939 \nL 236.193547 592.001529 \nL 280.042573 587.180917 \nL 323.891599 583.980851 \nL 367.740625 579.447424 \nL 411.589651 578.360222 \nL 455.438677 573.006266 \nL 499.287703 573.293451 \nL 543.136729 570.195952 \nL 586.985755 572.78062 \nL 630.834781 568.944644 \nL 674.683807 568.390786 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 624.802205 \nL 104.646469 595.693913 \nL 148.495495 573.970388 \nL 192.344521 566.667674 \nL 236.193547 554.564861 \nL 280.042573 538.277346 \nL 323.891599 533.477247 \nL 367.740625 518.748738 \nL 411.589651 504.533061 \nL 455.438677 494.666191 \nL 499.287703 486.132682 \nL 543.136729 458.706476 \nL 586.985755 439.383001 \nL 630.834781 426.890436 \nL 674.683807 407.977226 \n\" style=\"fill:none;stroke:#8c564b;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 630.751045 \nL 104.646469 605.47873 \nL 148.495495 591.406645 \nL 192.344521 587.303996 \nL 236.193547 584.309063 \nL 280.042573 580.883351 \nL 323.891599 576.534544 \nL 367.740625 576.144792 \nL 411.589651 580.657706 \nL 455.438677 590.134824 \nL 499.287703 582.70903 \nL 543.136729 587.980933 \nL 586.985755 593.006678 \nL 630.834781 596.801628 \nL 674.683807 592.104095 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 647.325745 \nL 104.646469 626.238132 \nL 148.495495 598.504227 \nL 192.344521 592.165635 \nL 236.193547 584.493682 \nL 280.042573 579.180752 \nL 323.891599 573.519097 \nL 367.740625 564.739429 \nL 411.589651 561.723982 \nL 455.438677 557.621334 \nL 499.287703 553.621251 \nL 543.136729 549.005772 \nL 586.985755 546.27751 \nL 630.834781 543.056931 \nL 674.683807 541.354332 \n\" style=\"fill:none;stroke:#e377c2;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path clip-path=\"url(#pd62ad9bee6)\" d=\"M 60.797443 650.07452 \nL 104.646469 632.597237 \nL 148.495495 605.396677 \nL 192.344521 600.86325 \nL 236.193547 595.693913 \nL 280.042573 591.75537 \nL 323.891599 587.20143 \nL 367.740625 580.247441 \nL 411.589651 578.606381 \nL 455.438677 576.57557 \nL 499.287703 575.590935 \nL 543.136729 574.893484 \nL 586.985755 575.693501 \nL 630.834781 571.754958 \nL 674.683807 572.02163 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 30.103125 675.718125 \nL 30.103125 376.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 705.378125 675.718125 \nL 705.378125 376.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 30.103125 675.718125 \nL 705.378125 675.718125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 30.103125 376.918125 \nL 705.378125 376.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_38\">\n    <!-- Accuracy: solid — test, dashed — train -->\n    <g transform=\"translate(250.042187 370.918125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 1281 1691 \nL 2994 1691 \nL 2138 3909 \nL 1281 1691 \nz\nM -38 0 \nL -38 331 \nL 372 331 \nL 2034 4666 \nL 2559 4666 \nL 4225 331 \nL 4684 331 \nL 4684 0 \nL 2988 0 \nL 2988 331 \nL 3506 331 \nL 3116 1356 \nL 1153 1356 \nL 763 331 \nL 1275 331 \nL 1275 0 \nL -38 0 \nz\n\" id=\"DejaVuSerif-41\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2266 3322 \nL 3341 3322 \nL 3341 331 \nL 3884 331 \nL 3884 0 \nL 2766 0 \nL 2766 588 \nQ 2606 256 2353 82 \nQ 2100 -91 1766 -91 \nQ 1213 -91 952 223 \nQ 691 538 691 1209 \nL 691 2988 \nL 172 2988 \nL 172 3322 \nL 1269 3322 \nL 1269 1388 \nQ 1269 781 1417 556 \nQ 1566 331 1947 331 \nQ 2347 331 2556 625 \nQ 2766 919 2766 1478 \nL 2766 2988 \nL 2266 2988 \nL 2266 3322 \nz\n\" id=\"DejaVuSerif-75\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSerif-41\"/>\n     <use x=\"72.216797\" xlink:href=\"#DejaVuSerif-63\"/>\n     <use x=\"128.222656\" xlink:href=\"#DejaVuSerif-63\"/>\n     <use x=\"184.228516\" xlink:href=\"#DejaVuSerif-75\"/>\n     <use x=\"248.632812\" xlink:href=\"#DejaVuSerif-72\"/>\n     <use x=\"296.435547\" xlink:href=\"#DejaVuSerif-61\"/>\n     <use x=\"356.054688\" xlink:href=\"#DejaVuSerif-63\"/>\n     <use x=\"412.060547\" xlink:href=\"#DejaVuSerif-79\"/>\n     <use x=\"468.554688\" xlink:href=\"#DejaVuSerif-3a\"/>\n     <use x=\"502.246094\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"534.033203\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"585.351562\" xlink:href=\"#DejaVuSerif-6f\"/>\n     <use x=\"645.556641\" xlink:href=\"#DejaVuSerif-6c\"/>\n     <use x=\"677.539062\" xlink:href=\"#DejaVuSerif-69\"/>\n     <use x=\"709.521484\" xlink:href=\"#DejaVuSerif-64\"/>\n     <use x=\"773.535156\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"805.322266\" xlink:href=\"#DejaVuSerif-2014\"/>\n     <use x=\"905.322266\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"937.109375\" xlink:href=\"#DejaVuSerif-74\"/>\n     <use x=\"977.294922\" xlink:href=\"#DejaVuSerif-65\"/>\n     <use x=\"1036.474609\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"1087.792969\" xlink:href=\"#DejaVuSerif-74\"/>\n     <use x=\"1127.978516\" xlink:href=\"#DejaVuSerif-2c\"/>\n     <use x=\"1159.765625\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"1191.552734\" xlink:href=\"#DejaVuSerif-64\"/>\n     <use x=\"1255.566406\" xlink:href=\"#DejaVuSerif-61\"/>\n     <use x=\"1315.185547\" xlink:href=\"#DejaVuSerif-73\"/>\n     <use x=\"1366.503906\" xlink:href=\"#DejaVuSerif-68\"/>\n     <use x=\"1430.908203\" xlink:href=\"#DejaVuSerif-65\"/>\n     <use x=\"1490.087891\" xlink:href=\"#DejaVuSerif-64\"/>\n     <use x=\"1554.101562\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"1585.888672\" xlink:href=\"#DejaVuSerif-2014\"/>\n     <use x=\"1685.888672\" xlink:href=\"#DejaVuSerif-20\"/>\n     <use x=\"1717.675781\" xlink:href=\"#DejaVuSerif-74\"/>\n     <use x=\"1757.861328\" xlink:href=\"#DejaVuSerif-72\"/>\n     <use x=\"1805.664062\" xlink:href=\"#DejaVuSerif-61\"/>\n     <use x=\"1865.283203\" xlink:href=\"#DejaVuSerif-69\"/>\n     <use x=\"1897.265625\" xlink:href=\"#DejaVuSerif-6e\"/>\n    </g>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 37.103125 488.23375 \nL 204.79375 488.23375 \nQ 206.79375 488.23375 206.79375 486.23375 \nL 206.79375 383.918125 \nQ 206.79375 381.918125 204.79375 381.918125 \nL 37.103125 381.918125 \nQ 35.103125 381.918125 35.103125 383.918125 \nL 35.103125 486.23375 \nQ 35.103125 488.23375 37.103125 488.23375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_97\">\n     <path d=\"M 39.103125 390.016562 \nL 59.103125 390.016562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_98\"/>\n    <g id=\"text_39\">\n     <!-- torch.nn.LSTM -->\n     <g transform=\"translate(67.103125 393.516562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-74\"/>\n      <use x=\"40.185547\" xlink:href=\"#DejaVuSerif-6f\"/>\n      <use x=\"100.390625\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"148.193359\" xlink:href=\"#DejaVuSerif-63\"/>\n      <use x=\"204.199219\" xlink:href=\"#DejaVuSerif-68\"/>\n      <use x=\"268.603516\" xlink:href=\"#DejaVuSerif-2e\"/>\n      <use x=\"300.390625\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"364.794922\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"429.199219\" xlink:href=\"#DejaVuSerif-2e\"/>\n      <use x=\"460.986328\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"527.392578\" xlink:href=\"#DejaVuSerif-53\"/>\n      <use x=\"595.898438\" xlink:href=\"#DejaVuSerif-54\"/>\n      <use x=\"662.597656\" xlink:href=\"#DejaVuSerif-4d\"/>\n     </g>\n    </g>\n    <g id=\"line2d_99\">\n     <path d=\"M 39.103125 404.694687 \nL 59.103125 404.694687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_100\"/>\n    <g id=\"text_40\">\n     <!-- RNNLayer -->\n     <g transform=\"translate(67.103125 408.194687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"75.292969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"250.292969\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"316.699219\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"376.318359\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"432.8125\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"491.992188\" xlink:href=\"#DejaVuSerif-72\"/>\n     </g>\n    </g>\n    <g id=\"line2d_101\">\n     <path d=\"M 39.103125 419.515 \nL 59.103125 419.515 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_102\"/>\n    <g id=\"text_41\">\n     <!-- RNNLayer + VarDO -->\n     <g transform=\"translate(67.103125 423.015)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"75.292969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"250.292969\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"316.699219\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"376.318359\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"432.8125\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"491.992188\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"539.794922\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"571.582031\" xlink:href=\"#DejaVuSerif-2b\"/>\n      <use x=\"655.371094\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"687.158203\" xlink:href=\"#DejaVuSerif-56\"/>\n      <use x=\"750.25\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"809.869141\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"857.671875\" xlink:href=\"#DejaVuSerif-44\"/>\n      <use x=\"937.847656\" xlink:href=\"#DejaVuSerif-4f\"/>\n     </g>\n    </g>\n    <g id=\"line2d_103\">\n     <path d=\"M 39.103125 434.335312 \nL 59.103125 434.335312 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_104\"/>\n    <g id=\"text_42\">\n     <!-- FastRNNLayer -->\n     <g transform=\"translate(67.103125 437.835312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-46\"/>\n      <use x=\"62.634766\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"122.253906\" xlink:href=\"#DejaVuSerif-73\"/>\n      <use x=\"173.572266\" xlink:href=\"#DejaVuSerif-74\"/>\n      <use x=\"213.757812\" xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"289.050781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"376.550781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"464.050781\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"530.457031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"590.076172\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"646.570312\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"705.75\" xlink:href=\"#DejaVuSerif-72\"/>\n     </g>\n    </g>\n    <g id=\"line2d_105\">\n     <path d=\"M 39.103125 449.155625 \nL 59.103125 449.155625 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_106\"/>\n    <g id=\"text_43\">\n     <!-- FastRNNLayer + VarDO -->\n     <g transform=\"translate(67.103125 452.655625)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-46\"/>\n      <use x=\"62.634766\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"122.253906\" xlink:href=\"#DejaVuSerif-73\"/>\n      <use x=\"173.572266\" xlink:href=\"#DejaVuSerif-74\"/>\n      <use x=\"213.757812\" xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"289.050781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"376.550781\" xlink:href=\"#DejaVuSerif-4e\"/>\n      <use x=\"464.050781\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"530.457031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"590.076172\" xlink:href=\"#DejaVuSerif-79\"/>\n      <use x=\"646.570312\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"705.75\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"753.552734\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"785.339844\" xlink:href=\"#DejaVuSerif-2b\"/>\n      <use x=\"869.128906\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"900.916016\" xlink:href=\"#DejaVuSerif-56\"/>\n      <use x=\"964.007812\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"1023.626953\" xlink:href=\"#DejaVuSerif-72\"/>\n      <use x=\"1071.429688\" xlink:href=\"#DejaVuSerif-44\"/>\n      <use x=\"1151.605469\" xlink:href=\"#DejaVuSerif-4f\"/>\n     </g>\n    </g>\n    <g id=\"line2d_107\">\n     <path d=\"M 39.103125 463.975937 \nL 59.103125 463.975937 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_108\"/>\n    <g id=\"text_44\">\n     <!-- HandmadeLSTM -->\n     <g transform=\"translate(67.103125 467.475937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-48\"/>\n      <use x=\"87.207031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"146.826172\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"211.230469\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"275.244141\" xlink:href=\"#DejaVuSerif-6d\"/>\n      <use x=\"370.068359\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"429.6875\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"493.701172\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"552.880859\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"619.287109\" xlink:href=\"#DejaVuSerif-53\"/>\n      <use x=\"687.792969\" xlink:href=\"#DejaVuSerif-54\"/>\n      <use x=\"754.492188\" xlink:href=\"#DejaVuSerif-4d\"/>\n     </g>\n    </g>\n    <g id=\"line2d_109\">\n     <path d=\"M 39.103125 478.654062 \nL 59.103125 478.654062 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_110\"/>\n    <g id=\"text_45\">\n     <!-- HandmadeLSTM + RecDO -->\n     <g transform=\"translate(67.103125 482.154062)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSerif-48\"/>\n      <use x=\"87.207031\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"146.826172\" xlink:href=\"#DejaVuSerif-6e\"/>\n      <use x=\"211.230469\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"275.244141\" xlink:href=\"#DejaVuSerif-6d\"/>\n      <use x=\"370.068359\" xlink:href=\"#DejaVuSerif-61\"/>\n      <use x=\"429.6875\" xlink:href=\"#DejaVuSerif-64\"/>\n      <use x=\"493.701172\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"552.880859\" xlink:href=\"#DejaVuSerif-4c\"/>\n      <use x=\"619.287109\" xlink:href=\"#DejaVuSerif-53\"/>\n      <use x=\"687.792969\" xlink:href=\"#DejaVuSerif-54\"/>\n      <use x=\"754.492188\" xlink:href=\"#DejaVuSerif-4d\"/>\n      <use x=\"856.884766\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"888.671875\" xlink:href=\"#DejaVuSerif-2b\"/>\n      <use x=\"972.460938\" xlink:href=\"#DejaVuSerif-20\"/>\n      <use x=\"1004.248047\" xlink:href=\"#DejaVuSerif-52\"/>\n      <use x=\"1079.541016\" xlink:href=\"#DejaVuSerif-65\"/>\n      <use x=\"1138.720703\" xlink:href=\"#DejaVuSerif-63\"/>\n      <use x=\"1194.726562\" xlink:href=\"#DejaVuSerif-44\"/>\n      <use x=\"1274.902344\" xlink:href=\"#DejaVuSerif-4f\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p43e57b6c5b\">\n   <rect height=\"298.8\" width=\"675.275\" x=\"30.103125\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pd62ad9bee6\">\n   <rect height=\"298.8\" width=\"675.275\" x=\"30.103125\" y=\"376.918125\"/>\n  </clipPath>\n </defs>\n</svg>\n","application/pdf":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Сделайте итоговые выводы о качестве работы моделей с разными реализациями DropOut:","metadata":{}},{"cell_type":"markdown","source":"**Ответ:**\n\n<font color=blue>Как видим, качество работы с дропаутом на тесте всегда лучше, чем без него (на трейне ситуация обратная), то есть он помогает не переобучаться во всех реализациях. Качество работы же отличается незначительно (сейчас лучше всего `FastRNN` с Variational Dropout), поэтому выбор модели здесь разумнее делать по времени — здесь очевидный лидер `FastRNN`.</font>","metadata":{}},{"cell_type":"markdown","source":"## Бонус. Zoneout (2 балла)","metadata":{}},{"cell_type":"markdown","source":"Это еще одна модификация идеи дропаута применительно к рекуррентным нейросетям. В Zoneout на каждом временном шаге с вероятностью p компонента скрытого состояния обновляется, а с вероятностью 1-p берется с предыдущего шага. \nВ Виде формул (m^t_h - бинарная маска):\n \n(сначала обычный рекуррентный переход, например LSTM)\n$$\ni = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\no = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n$$\n$$\nf = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \ng = tanh(h_{t-1} W^g + x_t U^g+b_g) \n$$\n$$\nc_t = f \\odot c_{t-1} +  i \\odot  g \\quad\nh_t =  o \\odot tanh(c_t) \\nonumber\n$$\nЗатем Zoneout:\n$$\nh_t = h_t * m_h^t + h_{t-1}*(1-m_h^t)\n$$\nВ этом методе маска уже должна быть разная во все моменты времени (иначе метод упрощается до дропаута Гала и Гарамани). На входы $x_t$ вновь можно накладывать маску до начала работы рекуррентного слоя.  \n\nЕсли у вас осталось время, вы можете реализовать этот метод. Выберите основу из трех рассмотренных случаев самостоятельно.","metadata":{}},{"cell_type":"markdown","source":"<font color=blue>За основу проще всего взять `RNNLayer`.</font>","metadata":{}},{"cell_type":"code","source":"class RNNLayerZone(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.0):\n        super().__init__()\n\n        self.dropout = dropout\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.rnn_cell = torch.nn.LSTMCell(self.input_size, self.hidden_size)\n    \n    def forward(self, input):\n        '''\n        Args:\n            input: tensor of shape (seq_len, batch, input_size).\n        Returns: output, (h_n, c_n)\n            output: tensor of shape (seq_len, batch, hidden_size)\n            h_n: tensor of shape (batch, hidden_size)\n            c_n: tensor of shape (batch, hidden_size)\n        '''\n        # Initialize h=h_0, c=c_0 of shape (batch, hidden_size)\n        h, c = init_h0_c0(input.size()[1], self.hidden_size, input)\n        \n        # Gen masks for input and hidden state\n        if self.dropout > 0.0:\n            # mx (input_size,)\n            mx = gen_dropout_mask(self.input_size, 0,\n                                      self.training, self.dropout, input)[0]\n            # masked_input (seq_len, batch, input_size).\n            masked_input = input * mx\n        else:\n            masked_input = input\n        \n        # Implement recurrent logic and return what nn.LSTM returns\n        # Do not forget to apply generated dropout masks!\n        output = []\n        for x in masked_input:\n            # x: (batch, input_size)\n            # h and c: (batch, hidden_size)\n            h_upd, c = self.rnn_cell(x, (h, c))\n            if self.dropout > 0.0:\n                mh = gen_dropout_mask(0, self.hidden_size,\n                                      self.training, self.dropout, input)[1]\n                h = h_upd * mh + h * (1 - mh)\n            else:\n                h = h_upd\n            output.append(h)\n            \n        output = torch.stack(output, dim=0)\n        return output, (h, c)","metadata":{"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"<font color=blue>В отсутствие дропаута слой эквивалентнен RNNLayer. Проверим, как работает модель с `dropout-0.25`.</font>","metadata":{}},{"cell_type":"code","source":"model = RNNClassifier(\n    embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=10,\n    vocab=vocab, rec_layer=RNNLayerZone, dropout=0.25,\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nstart = time()\nevals['RNNLayer + ZoneDO'] = train(train_dataloader, test_dataloader, model,\n                                        loss_fn, optimizer, device, num_epochs)\ntimes_do['RNNLayer + ZoneDO'] = time() - start","metadata":{"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"epoch 01/15; loss (train/test): 2.015/2.030; accuracy (train/test): 0.233/0.222\nepoch 02/15; loss (train/test): 1.797/1.826; accuracy (train/test): 0.336/0.319\nepoch 03/15; loss (train/test): 1.648/1.701; accuracy (train/test): 0.369/0.349\nepoch 04/15; loss (train/test): 1.561/1.632; accuracy (train/test): 0.389/0.364\nepoch 05/15; loss (train/test): 1.509/1.593; accuracy (train/test): 0.405/0.379\nepoch 06/15; loss (train/test): 1.466/1.572; accuracy (train/test): 0.420/0.389\nepoch 07/15; loss (train/test): 1.458/1.591; accuracy (train/test): 0.424/0.387\nepoch 08/15; loss (train/test): 1.401/1.538; accuracy (train/test): 0.444/0.399\nepoch 09/15; loss (train/test): 1.390/1.549; accuracy (train/test): 0.446/0.402\nepoch 10/15; loss (train/test): 1.355/1.549; accuracy (train/test): 0.457/0.406\nepoch 11/15; loss (train/test): 1.326/1.520; accuracy (train/test): 0.469/0.407\nepoch 12/15; loss (train/test): 1.304/1.531; accuracy (train/test): 0.474/0.408\nepoch 13/15; loss (train/test): 1.278/1.513; accuracy (train/test): 0.488/0.410\nepoch 14/15; loss (train/test): 1.277/1.538; accuracy (train/test): 0.483/0.409\nepoch 15/15; loss (train/test): 1.238/1.525; accuracy (train/test): 0.500/0.413\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=blue>По качеству снова видим преимущество использования дропаута (регуляризация), качество монотонно растет — accuracy сопоставима с другими подходами. Время работы сравним с почти аналогичным слоем:</font>","metadata":{}},{"cell_type":"code","source":"mtimes = times_do['RNNLayer + ZoneDO'] / times_do['RNNLayer + VarDO']\nprint(f\"Модель с RNNLayer с Zone Dropout обучается в {mtimes:.2f} \"\n      f\"раз дольше, чем с Variational Dropout\")","metadata":{"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Модель с RNNLayer с Zone Dropout обучается в 1.47 раз дольше, чем с Variational Dropout\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=blue>Время работы немного увеличивается из-за большего числа генерируемых масок.</font>","metadata":{}},{"cell_type":"markdown","source":"# Часть 2. Language Modeling с помощью LSTM. (3 балла)","metadata":{"ExecuteTime":{"end_time":"2021-03-31T16:05:00.702763Z","start_time":"2021-03-31T16:05:00.674835Z"}}},{"cell_type":"markdown","source":"Во второй части мы попробуем обучить модель для генерации отзывов по их началу.","metadata":{}},{"cell_type":"markdown","source":"Концептуально модель будет выглядеть следующим образом:\n    \n![image info](https://blog.feedly.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-12.08.35-PM.png)","metadata":{}},{"cell_type":"markdown","source":"В процессе обучения будем тренировать сеть предсказывать вероятность следующего символа при условии всех предыдущих. Эту вероятность можно моделировать с помощью скрытого состояния $h^{(t)}$ пропуская его через линейный слой с выходной размерностью равной размерности словаря:\n$$\np(x^{t}|x^{t-1}, ..., x^{1}) = SoftMax(Linear(h^{(t)}))\n$$","metadata":{}},{"cell_type":"markdown","source":"Обратите внимание, что для вычисления $p(x^{t}|x^{t-1}, ..., x^{1})$ для всех моментов времени достаточно сделать один проход по RNN, а затем применить линейное преобразование ко всем скрытым состояниям.","metadata":{}},{"cell_type":"markdown","source":"В качестве функции потерь необходимо использовать `CrossEntropy`.","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:37:56.10052Z","start_time":"2021-04-02T00:37:56.072747Z"}}},{"cell_type":"markdown","source":"Рассмотрим другой важный момент. Для того, чтобы решить данную задачу, модель должна уметь определять момент начала генерации предложения и оповещать о завершении генерации -- конце предложения. Для этого добавим в словарь вспомогательные токены `<sos>`, `<eos>`. Добавив `<sos>` в начало каждого предложения и `<eos>` в конец.\n\nМодель сможет начинать генерацию как только ей будет передан токен `<sos>` и заканчивать генерацию, как только на очередном месте самым вероятным токеном оказывается `<eos>`.","metadata":{}},{"cell_type":"markdown","source":"Для решения этой задачи мы воспользуемся уже реализованной LSTM с дропаутом `FastRNNLayer` и классом `RNNClassifier`, то есть архитектура сети принципиально не поменяется. ","metadata":{}},{"cell_type":"markdown","source":"## Реализация модели и цикла обучения (1 балл)","metadata":{}},{"cell_type":"markdown","source":"**Не используйте циклы в `RNNLM`, `LMCrossEntropyLoss`, `LMAccuracy`**","metadata":{}},{"cell_type":"code","source":"class RNNLM(RNNClassifier):\n    def __init__(\n        self, embedding_dim, hidden_dim, vocab, dropout=0.5, layers_dropout=0.5, num_layers=1\n    ):\n        super().__init__(\n            embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_size=len(vocab), vocab=vocab,\n            rec_layer=FastRNNLayer, dropout=dropout, layers_dropout=layers_dropout, num_layers=num_layers\n        )\n    \n    def forward(self, tokens, tokens_lens):\n        \"\"\"\n        :param torch.tensor(dtype=torch.long) tokens: Batch of texts represented with tokens. Shape: [T, B]\n        :param torch.tensor(dtype=torch.long) tokens_lens: Number of non-padding tokens for each object in batch. Shape: [B]\n        :return torch.tensor: Distribution of next token for each time step. Shape: [T, B, V], V -- size of vocabulary\n        \"\"\"\n        # Make embeddings for all tokens\n        output = self.word_embeddings(tokens)  # (T, B, embedding_dim)\n        \n        # Forward pass embeddings through network\n        output = self.rnn(output)[0]  # (T, B, hidden_dim)\n        \n        # Take all hidden states from the last layer of LSTM for each step and perform linear transformation\n        output = self.lin(output)  # (T, B, len(vocab))\n        return output","metadata":{"ExecuteTime":{"end_time":"2021-04-02T02:07:02.815198Z","start_time":"2021-04-02T02:07:02.787445Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Реализуем функцию потерь для данной задачи. \n\nМоменты на которые нужно обратить внимание:\n1. Распределение вероятности следующего токена для последнего токена в последовательности не участвует в подсчёте функции потерь.\n2. Необходимо учитывать, что в одном батче могут быть тексты разной длины.","metadata":{}},{"cell_type":"markdown","source":"Для решения второй проблемы можно воспользоваться функцией `torch.nn.utils.rnn.pack_padded_sequence`. \n\nПринимая на вход батч тензоров и длину каждого тензора без учёта паддинга эта функция позволяет получить все элементы в тензорах, которые не относятся к паддингу в виде плоского массива:","metadata":{}},{"cell_type":"code","source":"padded_tensors = torch.tensor([\n    [[1, 11, 111], [2, 22, 222], [3, 33, 333]],\n    [[4, 44, 444], [5, 55, 555], [6, 66, 666]],\n    [[7, 77, 777], [0, 0, 0], [8, 88, 888]],\n    [[9, 99, 999], [0, 0, 0], [0, 0, 0]]\n])\ntensors_lens = torch.tensor([4, 2, 3])","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:54:40.004897Z","start_time":"2021-04-02T00:54:39.977287Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"padded_tensors.shape","metadata":{"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 3, 3])"},"metadata":{}}]},{"cell_type":"markdown","source":"Обратите внимание, что `torch.nn.utils.rnn.pack_padded_sequence` автоматически переупорядочивает тензоры в батче по убыванию их длины.","metadata":{}},{"cell_type":"code","source":"torch.nn.utils.rnn.pack_padded_sequence(padded_tensors, tensors_lens, batch_first=False, enforce_sorted=False)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:54:24.517023Z","start_time":"2021-04-02T00:54:24.490588Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"PackedSequence(data=tensor([[  1,  11, 111],\n        [  3,  33, 333],\n        [  2,  22, 222],\n        [  4,  44, 444],\n        [  6,  66, 666],\n        [  5,  55, 555],\n        [  7,  77, 777],\n        [  8,  88, 888],\n        [  9,  99, 999]]), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=tensor([0, 2, 1]), unsorted_indices=tensor([0, 2, 1]))"},"metadata":{}}]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence\n\ndef pack_it(TB_tensor, tokens_lens):\n    # adjust tokens_lens: discard <sos>, consider <eos>\n    return pack_padded_sequence(TB_tensor, tokens_lens.cpu() + 1,\n                                batch_first=False, enforce_sorted=False).data\n\nclass LMCrossEntropyLoss(torch.nn.CrossEntropyLoss):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n    def forward(self, outputs, tokens, tokens_lens):\n        \"\"\"\n        :param torch.tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n        :param torch.tensor tokens: Batch of tokens. Shape: [T, B]\n        :param torch.tensor tokens_lens: Length of each sequence in batch\n        :return torch.tensor: CrossEntropyLoss between corresponding logits and tokens\n        \"\"\"\n        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n        # Do not forget specify enforce_sorted=False and correct value of batch_first \n        packed_outputs = pack_it(outputs, tokens_lens)\n        packed_tokens = pack_it(tokens[1:], tokens_lens)\n        \n        # Use super().forward(..., ...) to compute CrossEntropyLoss\n        loss = super().forward(packed_outputs, packed_tokens) \n        return loss","metadata":{"ExecuteTime":{"end_time":"2021-04-02T02:07:06.289671Z","start_time":"2021-04-02T02:07:06.262883Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"Для оценки качества нам также необходимо вычислять долю правильно предсказанных токенов. Реализуйте класс для вычисления точности.","metadata":{}},{"cell_type":"code","source":"class LMAccuracy(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, outputs, tokens, tokens_lens):\n        \"\"\"\n        :param torch.tensor outputs: Output from RNNLM.forward. Shape: [T, B, V]\n        :param torch.tensor tokens: Batch of tokens. Shape: [T, B]\n        :param torch.tensor tokens_lens: Length of each sequence in batch\n        :return torch.tensor: Accuracy for given logits and tokens\n        \"\"\"\n        # Use torch.nn.utils.rnn.pack_padded_sequence().data to remove padding and flatten logits and tokens\n        # Do not forget specify enforce_sorted=False and correct value of batch_first \n        # YOUR CODE HERE\n        packed_outputs = pack_it(outputs, tokens_lens)\n        packed_tokens = pack_it(tokens[1:], tokens_lens)\n        \n        return (packed_outputs.argmax(axis=1) == packed_tokens).sum()","metadata":{"ExecuteTime":{"end_time":"2021-04-02T02:07:07.335981Z","start_time":"2021-04-02T02:07:07.309586Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"Модифицируйте функции `train_epoch`, `evaluate`, `train` для обучения LM.\n\n**При вычислении точности, обратите внимание на то, что мы не предсказываем первый токен в каждой последовательности и токены, относящиеся к паддингу.**","metadata":{}},{"cell_type":"code","source":"def extract_batch_lm(data, device):\n    return [data[key].to(device) for key in ['tokens', 'tokens_lens']]\n\ndef train_epoch_lm(dataloader, model, loss_fn, optimizer, device):\n    model.train()\n    for idx, data in enumerate(dataloader):\n        tokens, tokens_lens = extract_batch_lm(data, device)\n        model.zero_grad()\n        output = model(tokens, tokens_lens)\n        loss = loss_fn(output, tokens, tokens_lens)\n        loss.backward()\n        optimizer.step()\n    \ndef evaluate_lm(dataloader, model, loss_fn, device):\n    model.eval()\n    \n    total_tokens = 0\n    total_loss, total_acc = 0.0, 0\n    \n    accuracy_fn = LMAccuracy()\n    with torch.no_grad():\n        for idx, data in enumerate(dataloader):\n            tokens, tokens_lens = extract_batch_lm(data, device)\n            output = model(tokens, tokens_lens)\n            loss = loss_fn(output, tokens, tokens_lens).item()\n            acc = accuracy_fn(output, tokens, tokens_lens).item()\n            n_tokens = (tokens_lens + 1).sum().item()\n            total_loss += loss * n_tokens\n            total_acc += acc\n            total_tokens += n_tokens\n    return total_loss / total_tokens, total_acc / total_tokens\n\ndef train_lm(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs):\n    train_losses, test_losses = [], []\n    train_accs, test_accs = [], []\n    for epoch in range(1, num_epochs+1):\n        train_epoch_lm(train_loader, model, loss_fn, optimizer, device)\n        train_loss, train_acc = evaluate_lm(train_loader, model, loss_fn, device)\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        test_loss, test_acc = evaluate_lm(test_loader, model, loss_fn, device)\n        test_losses.append(test_loss)\n        test_accs.append(test_acc)\n        print(f'epoch {epoch:02}/{num_epochs};', end=' ')\n        print(f'loss (train/test): {train_loss:.3f}/{test_loss:.3f};', end=' ')\n        print(f'accuracy (train/test): {train_acc:.3f}/{test_acc:.3f}')\n    return train_losses, train_accs, test_losses, test_accs","metadata":{"ExecuteTime":{"end_time":"2021-04-02T02:07:31.492984Z","start_time":"2021-04-02T02:07:31.459655Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"Теперь у нас всё готово для обучения модели.","metadata":{}},{"cell_type":"markdown","source":"Создадим словарь с `<sos>`, `<eos>` токенами.\n\nОбратите внимание, что в отличие от классификации текстов нам необходимо значительно увеличить размер словаря, чтобы доля `<unk>` токенов была не велика.\n\nТак же, так как задача генерации значительно сложнее задачи классификации текстов будем обучать модель только на префиксах рецензий длины $20$. Это позволяет значительно ускорить обучение.","metadata":{"ExecuteTime":{"end_time":"2021-04-02T01:06:12.73618Z","start_time":"2021-04-02T01:06:12.708814Z"}}},{"cell_type":"code","source":"specials = ['<pad>', '<unk>', '<sos>', '<eos>']\nfor special in specials:\n    counter[special] = 0\nlm_vocab = torchtext.vocab.Vocab(counter, specials=specials, specials_first=True, max_size=30000)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:06:20.093645Z","start_time":"2021-04-02T00:06:19.926668Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"lm_test_dataset = LargeMovieReviewDataset(test_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)\nlm_train_dataset = LargeMovieReviewDataset(train_data_path, lm_vocab, max_len=20, pad_sos=True, pad_eos=True)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T00:06:58.566893Z","start_time":"2021-04-02T00:06:21.430692Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"Создадим даталоадеры для тестовой и обучающей выборок:","metadata":{}},{"cell_type":"code","source":"lm_test_dataloader = DataLoader(\n    lm_test_dataset, batch_size=96, shuffle=False, num_workers=3, \n    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n)\nlm_train_dataloader = DataLoader(\n    lm_train_dataset, batch_size=96, shuffle=True, num_workers=3, \n    collate_fn=partial(collate_fn, padding_value=lm_vocab.lookup_indices(['<pad>'])[0])\n)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T12:29:16.213723Z","start_time":"2021-04-02T12:29:16.186954Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"Убедитесь, что все предложения имеют в начале `<sos>` токен, а в конце -- `<eos>` токен.","metadata":{}},{"cell_type":"code","source":"batch = next(iter(lm_train_dataloader))\nbatch['tokens'], batch['tokens_lens']","metadata":{"ExecuteTime":{"end_time":"2021-04-02T12:29:17.218115Z","start_time":"2021-04-02T12:29:16.922801Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"(tensor([[    2,     2,     2,  ...,     2,     2,     2],\n         [   22,    43,   948,  ...,    36,   585,    15],\n         [ 4237,   318,  1080,  ...,    41,  1054,   836],\n         ...,\n         [   69,     1,   210,  ...,  6017,   525,     1],\n         [11376,    30,   110,  ...,   549, 13321,   273],\n         [    3,     3,     3,  ...,     3,     3,     3]]),\n tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 20, 20,\n         20, 20, 20, 20, 20, 20]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Создадим модель, функцию потерь и оптимизатор: ","metadata":{}},{"cell_type":"code","source":"lm_model = RNNLM(\n    embedding_dim=512, hidden_dim=512, vocab=lm_vocab, dropout=0.6, layers_dropout=0.6, num_layers=2\n).to(device=device)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T14:15:32.916424Z","start_time":"2021-04-02T14:15:32.525452Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"lm_loss_fn = LMCrossEntropyLoss(reduction='mean')\nlm_optimizer = torch.optim.Adam(lm_model.parameters(), lr=0.005, weight_decay=1.2e-6)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T14:15:33.332806Z","start_time":"2021-04-02T14:15:33.307749Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Обучим модель:","metadata":{}},{"cell_type":"code","source":"%%time\nlm_train_losses, lm_train_accs, lm_test_losses, lm_test_accs = train_lm(\n    lm_train_dataloader, lm_test_dataloader, lm_model, lm_loss_fn, lm_optimizer, device, 10\n)","metadata":{"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"epoch 01/10; loss (train/test): 7.625/7.750; accuracy (train/test): 0.070/0.070\nepoch 02/10; loss (train/test): 7.418/7.614; accuracy (train/test): 0.079/0.078\nepoch 03/10; loss (train/test): 7.150/7.445; accuracy (train/test): 0.089/0.088\nepoch 04/10; loss (train/test): 6.961/7.380; accuracy (train/test): 0.096/0.093\nepoch 05/10; loss (train/test): 6.753/7.321; accuracy (train/test): 0.102/0.097\nepoch 06/10; loss (train/test): 6.577/7.306; accuracy (train/test): 0.106/0.099\nepoch 07/10; loss (train/test): 6.418/7.309; accuracy (train/test): 0.109/0.100\nepoch 08/10; loss (train/test): 6.270/7.322; accuracy (train/test): 0.114/0.102\nepoch 09/10; loss (train/test): 6.150/7.353; accuracy (train/test): 0.119/0.103\nepoch 10/10; loss (train/test): 6.024/7.388; accuracy (train/test): 0.121/0.102\nCPU times: user 4min 51s, sys: 7.55 s, total: 4min 59s\nWall time: 5min 14s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Реализация декодера. (2 балла)","metadata":{}},{"cell_type":"markdown","source":"Теперь, реализуем последнюю деталь -- декодирование с использованием обученной модели.\nЕсть несколько вариантов. Рассмотрим два самых простых:\n1. **Жадное декодирование.** На каждом шаге мы выбираем токен с максимальной вероятностью и используем его для обновления скрытого состояния RNN.\n2. **Top-k sampling.** На очередном шаге рассматриваются $k$ токенов с самыми большими вероятностями. Остальные токены игнорируются. Из выбранных токенов семплируется следующий токен пропорционально их вероятностям.\n\nПрочитать подробнее про разные варианты декодирования можно по ссылкам:\n1. [От huggingface](https://huggingface.co/blog/how-to-generate)\n2. [На towardsdatascience](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)","metadata":{}},{"cell_type":"markdown","source":"Существенным в процессе декодирования является критерий останова. Как только очередной самый вероятный символ оказался `<eos>`, то данная последовательность считается сгенерированной. Однако, может так оказаться, что `<eos>` никогда не будет выбран, тогда необходимо прекратить генерацию, как только длина последовательности перейдёт порог `max_generated_len`.","metadata":{}},{"cell_type":"code","source":"def decode(model, start_tokens, start_tokens_lens, max_generated_len=20, top_k=None):\n    \"\"\"\n    :param RNNLM model: Model\n    :param torch.tensor start_tokens: Batch of seed tokens. Shape: [T, B]\n    :param torch.tensor start_tokens_lens: Length of each sequence in batch. Shape: [B]\n    :return Tuple[torch.tensor, torch.tensor]. Newly predicted tokens and length of generated part. Shape [T*, B], [B]\n    \"\"\"\n    # Get embedding for start_tokens\n    embedding = model.word_embeddings(start_tokens)\n    \n    # Pass embedding through rnn and collect hidden states and cell states for each time moment\n    all_h, all_c = [], []\n    h = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n    c = embedding.new_zeros([model.rnn.num_layers, start_tokens.shape[1], model.hidden_dim])\n    for time_step in range(start_tokens.shape[0]):\n        h, c = model.rnn(embedding[time_step].unsqueeze(0), (h, c))[1]\n        all_h.append(h)\n        all_c.append(c)\n    all_h = torch.stack(all_h, dim=1)\n    all_c = torch.stack(all_c, dim=1)\n    # Take final hidden state and cell state for each start sequence in batch\n    # We will use them as h_0, c_0 for generation new tokens\n    h = all_h[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n    c = all_c[:, start_tokens_lens - 1, torch.arange(start_tokens_lens.shape[0])]\n\n    # List of predicted tokens for each time step\n    predicted_tokens = []\n    # Length of generated part for each object in the batch\n    decoded_lens = torch.zeros_like(start_tokens_lens, dtype=torch.long)\n    # Boolean mask where we store if the sequence has already generated\n    # i.e. `<eos>` was selected on any step\n    is_finished_decoding = torch.zeros_like(start_tokens_lens, dtype=torch.bool)\n    \n    # Stop when all sequences in the batch are finished\n    while not torch.all(is_finished_decoding) and torch.max(decoded_lens) < max_generated_len:\n        # Evaluate next token distribution using hidden state h.\n        # Note. Over first dimension h has hidden states for each layer of LSTM.\n        #     We must use hidden state from the last layer\n        # YOUR CODE HERE\n        logits = model.lin(h[1])  # (B, len(vocab))\n\n        if top_k is not None:\n            # Top-k sampling. Use only top-k most probable logits to sample next token\n            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n            # Mask non top-k logits\n            logits[indices_to_remove] = -1e10\n            # Sample next_token. \n            next_token = torch.distributions.categorical.Categorical(logits=logits).sample()\n        else:\n            # Select most probable token\n            next_token = logits.argmax(dim=1)\n\n        predicted_tokens.append(next_token)\n\n        decoded_lens += (~is_finished_decoding)\n        is_finished_decoding |= (next_token == torch.tensor(model.vocab.lookup_indices(['<eos>'])[0]))\n\n        # Evaluate embedding for next token\n        embedding = model.word_embeddings(next_token)\n\n        # Update hidden and cell states\n        h, c = model.rnn(embedding.unsqueeze(0), (h, c))[1]\n    \n    return torch.stack(predicted_tokens), decoded_lens","metadata":{"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"Попробуем сгенерировать продолжения для нескольких префиксов:","metadata":{"ExecuteTime":{"end_time":"2021-04-02T01:38:06.232189Z","start_time":"2021-04-02T01:38:06.205413Z"}}},{"cell_type":"code","source":"start_tokens = torch.tensor([\n    lm_model.vocab.lookup_indices(['<sos>', '<pad>', '<pad>', '<pad>']),\n    lm_model.vocab.lookup_indices(['<sos>', 'my', 'favorite', 'movie']),\n    lm_model.vocab.lookup_indices(['<sos>', 'the', 'best', 'movie']),\n    lm_model.vocab.lookup_indices(['<sos>', 'the', 'worst', 'movie']),\n]).T\n\nstart_tokens_lens = torch.tensor([1, 4, 4, 4])","metadata":{"ExecuteTime":{"end_time":"2021-04-02T14:28:22.568613Z","start_time":"2021-04-02T14:28:22.54581Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"lm_model = lm_model.cpu()\nlm_model.eval()\ndecoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens, max_generated_len=20, top_k=5)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T14:28:28.222137Z","start_time":"2021-04-02T14:28:27.930196Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"for text_idx in range(start_tokens.shape[1]):\n    decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n    tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist() + decoded_text_tokens.tolist()\n    words = np.array(lm_model.vocab.itos)[np.array(tokens)]\n    print(' '.join(words))","metadata":{"ExecuteTime":{"end_time":"2021-04-02T14:28:28.75138Z","start_time":"2021-04-02T14:28:28.708461Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"<sos> movie bad movie bad acting bad script bad writing bad bad bad bad even worse waste time money bad movie\n<sos> <unk> favorite movie time seventies <unk> one movies made one best thrillers ever made one worst movies ever seen acting bad bad bad\n<sos> <unk> best movie ever seen bad bad bad acting bad bad good bad bad acting bad directing bad script bad script bad dialogue\n<sos> <unk> worst movie ever seen entire life movie bad acting good bad acting bad writing even worse bad bad <eos>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Попробуйте выполнить семплирование для разных $k$. Сравните результаты top-k семплирования с жадным декодированием. Опишите ваши наблюдения.","metadata":{}},{"cell_type":"markdown","source":"<font color=blue>Обернем вопрос в функцию...</font>","metadata":{}},{"cell_type":"code","source":"def test_topk(topk):\n    decoded_tokens, decoded_lens = decode(lm_model, start_tokens, start_tokens_lens,\n                                          max_generated_len=20, top_k=topk)\n    for text_idx in range(start_tokens.shape[1]):\n        decoded_text_tokens = decoded_tokens[:decoded_lens[text_idx], text_idx]\n        tokens = start_tokens[:start_tokens_lens[text_idx], text_idx].tolist() + decoded_text_tokens.tolist()\n        words = np.array(lm_model.vocab.itos)[np.array(tokens)]\n        print(' '.join(words))","metadata":{"ExecuteTime":{"end_time":"2021-04-02T17:06:36.404126Z","start_time":"2021-04-02T17:06:36.399654Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"<font color=blue>Жадное декодирование:</font>","metadata":{}},{"cell_type":"code","source":"test_topk(None)","metadata":{"trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"<sos> movie bad acting bad acting bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad\n<sos> <unk> favorite movie time one best movies ever seen movie ever seen movie bad acting bad bad bad bad bad bad bad bad\n<sos> <unk> best movie ever seen movie bad acting bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad\n<sos> <unk> worst movie ever seen bad acting bad acting bad directing bad directing bad script bad acting bad directing bad script bad acting\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(1, 12, 2):\n    print(f'=======top_k={i}=======')\n    test_topk(i)\n    print('\\n')","metadata":{"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"=======top_k=1=======\n<sos> movie bad acting bad acting bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad\n<sos> <unk> favorite movie time one best movies ever seen movie ever seen movie bad acting bad bad bad bad bad bad bad bad\n<sos> <unk> best movie ever seen movie bad acting bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad\n<sos> <unk> worst movie ever seen bad acting bad acting bad directing bad directing bad script bad acting bad directing bad script bad acting\n\n\n=======top_k=3=======\n<sos> movie really bad bad bad bad movies bad acting bad bad bad bad bad bad movie bad script bad acting\n<sos> <unk> favorite movie ever seen life movie really bad bad bad bad bad movie bad bad bad bad bad bad bad bad bad\n<sos> <unk> best movie time seen long time see one best movies ever saw one movies ever seen movie bad bad acting bad bad\n<sos> <unk> worst movie ever seen seen one best movies ever seen one bad movies seen long money movie bad acting terrible acting bad\n\n\n=======top_k=5=======\n<sos> movie bad bad bad bad movies bad bad bad acting bad bad bad movie really bad bad movies bad bad\n<sos> <unk> favorite movie ever since seen since loved <unk> movie <unk> <unk> <unk> <unk> <unk> movie <unk> <unk> <unk> one <unk> movie <unk>\n<sos> <unk> best movie ever seen movie bad acting bad acting bad bad directing bad writing good direction bad direction bad script <eos>\n<sos> <unk> worst movie ever seen bad acting bad bad bad script horrible direction terrible direction bad acting bad script bad acting bad directing\n\n\n=======top_k=7=======\n<sos> first movie bad movie really bad bad bad bad funny bad bad movie bad plot bad acting bad writing cheesy\n<sos> <unk> favorite movie buff watch movie first time ever seen saw one times really bad good movie like movie <eos>\n<sos> <unk> best movie made movie <unk> ever seen movie <unk> movie ever seen movie bad acting bad script bad bad acting bad bad\n<sos> <unk> worst movie ever seen entire time watching bad movie like like good movie watch movie bad acting bad acting <eos>\n\n\n=======top_k=9=======\n<sos> saw first movie came across dvd cable cable tv tv movie festival one favorite movies ever seen seen since loved\n<sos> <unk> favorite movie buff seen one night tv channel tv series thought would good laugh something else good movie would think movie great\n<sos> <unk> best movie ever made one funniest movies ever seen acting terrible acting bad bad acting bad writing bad script <eos>\n<sos> <unk> worst movie seen date one best movies ever seen life movie even though even worse bad movie bad bad good bad gangster\n\n\n=======top_k=11=======\n<sos> saw movie yesterday watched twice movie yesterday came across release loved movie really like movie much better film movie <unk>\n<sos> <unk> favorite movie one favorite movies time seventies one favorite movies time one best movies seen year old girl <unk> <unk> <eos>\n<sos> <unk> best movie ever seen entire time movie terrible bad acting bad good bad script bad story bad acting bad story one bad\n<sos> <unk> worst movie ever seen movie bad acting bad good acting bad script badly plot badly seen many many times one movies <eos>\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Ответ:**","metadata":{}},{"cell_type":"markdown","source":"* <font color=blue>Как и следовало ожидать, семплирование с `top_k=1` эквивалентно жадному</font>\n* <font color=blue>С меньшим числом `top_k` тексты генерируются с незавидным разнообразием токенов (bad bAd BAd)</font>\n* <font color=blue>С большим числом `top_k` язык значительно богаче</font>\n* <font color=blue>Во всех случаях в 3-м примере нарушается логика при генерации (на входе хвалим, на выходе — худший фильм)</font>\n* <font color=blue>Модель тоже любит критиковать кино</font>","metadata":{}},{"cell_type":"markdown","source":"## Бонус. Cущественное улучшение качества. (до 3 баллов)","metadata":{"ExecuteTime":{"end_time":"2021-04-02T15:04:51.67826Z","start_time":"2021-04-02T15:04:51.673587Z"}}},{"cell_type":"markdown","source":"Та модель, которая использовалась в предыдущей части во многом заимствует улучшения LSTM из статьи [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf). Вы можете попробовать применить другие варианты регуляризации из данной статьи для существенного улучшения качества LM.\n\nНапример:\n1. Dropout для эмбеддингов\n2. Dropout входов и выходов RNN\n3. Регуляризация активаций (AR/TAR)\n4. NT-ASGD\n5. Tied веса эмбеддингов и софтмакса","metadata":{}}]}